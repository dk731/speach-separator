{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 22:36:54.303940: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-30 22:36:55.025019: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-04-30 22:36:55.871900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-30 22:36:55.925306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-30 22:36:55.925449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import kaldiio\n",
    "\n",
    "from scipy.signal import welch\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "tf.config.list_physical_devices(\"GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = os.getenv(\"PROJECT_ROOT\")\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "CLIPS_PATH = os.getenv(\"CLIPS_PATH\")\n",
    "\n",
    "VALIDATED_LIST_PATH = os.path.join(os.getenv(\"CLIPS_META_PATH\"), \"validated.tsv\")\n",
    "XVECTOR_RESULT_PATH = os.getenv(\"XVECTOR_RESULT_PATH\")\n",
    "XVECTOR_SCP_PATH = os.path.join(XVECTOR_RESULT_PATH, \"xvector.scp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_xvectors = kaldiio.load_scp(XVECTOR_SCP_PATH)\n",
    "valid_speakers = set(speakers_xvectors.keys())\n",
    "\n",
    "raw_clips_meta = pd.read_table(VALIDATED_LIST_PATH)\n",
    "raw_clips_meta = raw_clips_meta[raw_clips_meta[\"client_id\"].isin(valid_speakers)]\n",
    "\n",
    "\n",
    "def get_path(row):\n",
    "    return f\"{CLIPS_PATH}/{row}.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    Concatenate,\n",
    "    LeakyReLU,\n",
    "    BatchNormalization,\n",
    "    Flatten,\n",
    "    AveragePooling2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " spectrogram_input (InputLayer)  [(None, 65, 384, 1)  0          []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 65, 384, 32)  320         ['spectrogram_input[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 65, 384, 32)  128        ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " average_pooling2d_9 (AveragePo  (None, 32, 192, 32)  0          ['batch_normalization_9[0][0]']  \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 192, 64)  18496       ['average_pooling2d_9[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 32, 192, 64)  256        ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " average_pooling2d_10 (AverageP  (None, 16, 96, 64)  0           ['batch_normalization_10[0][0]'] \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 16, 96, 128)  73856       ['average_pooling2d_10[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 16, 96, 128)  512        ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " average_pooling2d_11 (AverageP  (None, 8, 48, 128)  0           ['batch_normalization_11[0][0]'] \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 49152)        0           ['average_pooling2d_11[0][0]']   \n",
      "                                                                                                  \n",
      " x_vector_input (InputLayer)    [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 49664)        0           ['flatten_2[0][0]',              \n",
      "                                                                  'x_vector_input[0][0]']         \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 128)          6357120     ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 128)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 256)          33024       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 256)          0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 512)          131584      ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 384)          196992      ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,812,288\n",
      "Trainable params: 6,811,840\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "HYPER_PARAMS = {\n",
    "    # Model parameters\n",
    "    \"window-width\": 65,\n",
    "    \"x-vector-dim\": 512,\n",
    "    # Training parameters\n",
    "    \"batch-size\": 256,\n",
    "    \"epochs\": 10,\n",
    "    \"learning-rate\": 2e-5,\n",
    "    \"learn-test-split\": 0.8,\n",
    "    \"logs-batch-frequency\": 25,\n",
    "    # FFT parameters\n",
    "    \"nfft\": 4096,\n",
    "    \"fft-window\": 2048,\n",
    "    \"fft-stride\": 256,\n",
    "    # General\n",
    "    \"min-frequency\": 0,\n",
    "    \"max-frequency\": 9e3, # Crop input audio spectrogram to this frequency\n",
    "    \"audio-rate\": 48e3,\n",
    "    # Data augmentation possibilities\n",
    "    # \"augment-raw\": 0.05,\n",
    "    # \"augment-empty-noise\": 0.05,\n",
    "    # \"augment-noise\": 0.2,\n",
    "    # \"augment-dialogue\": 0.4,\n",
    "    # \"augment-dialogue-noise\": 0.45,\n",
    "    \n",
    "    \"augment-raw\": 1,\n",
    "    \"augment-empty-noise\": 1,\n",
    "    \"augment-noise\": 1,\n",
    "    \"augment-dialogue\": 1,\n",
    "    \"augment-dialogue-noise\": 1,\n",
    "    # Data augmentation parameters\n",
    "    \"noise-min-level\": 0.001,\n",
    "    \"noise-max-level\": 0.008,\n",
    "    \"dialogue-min-speakers\": 1,\n",
    "    \"dialogue-max-speakers\": 6,\n",
    "    \"dialogue-normalization-factor\": 0.8, # 0 - means no normalization (samples are added as is), 1 - means full normalization (final samples will be divided by number of speakers)\n",
    "}\n",
    "\n",
    "HYPER_PARAMS[\"window-height\"] = int(\n",
    "    (HYPER_PARAMS[\"max-frequency\"] - HYPER_PARAMS[\"min-frequency\"])\n",
    "    / HYPER_PARAMS[\"audio-rate\"]\n",
    "    * (HYPER_PARAMS[\"nfft\"] // 2 + 1)\n",
    ")\n",
    "\n",
    "# Model parameters\n",
    "mel_spectrogram_shape = (\n",
    "    HYPER_PARAMS[\"window-width\"],\n",
    "    HYPER_PARAMS[\"window-height\"],\n",
    "    1,\n",
    ")  # Replace window_size and num_mel_bands with your values\n",
    "\n",
    "# Leaky ReLU activation function\n",
    "leaky_relu = LeakyReLU(alpha=0.2)\n",
    "\n",
    "# Mel-spectrogram input\n",
    "mel_spectrogram_input = Input(shape=mel_spectrogram_shape, name=\"spectrogram_input\")\n",
    "\n",
    "# Convolutional layers\n",
    "x = Conv2D(16, (3, 3), padding=\"same\", activation=leaky_relu)(mel_spectrogram_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Convolutional layers\n",
    "x = Conv2D(32, (3, 3), padding=\"same\", activation=leaky_relu)(mel_spectrogram_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), padding=\"same\", activation=leaky_relu)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(128, (3, 3), padding=\"same\", activation=leaky_relu)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "# X-vector input\n",
    "x_vector_input = Input(shape=(HYPER_PARAMS[\"x-vector-dim\"],), name=\"x_vector_input\")\n",
    "\n",
    "# Concatenate flattened CNN output with x-vector input\n",
    "combined_input = Concatenate()([x, x_vector_input])\n",
    "\n",
    "# Dense layers\n",
    "y = Dense(128, activation=leaky_relu)(combined_input)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Dense(256, activation=leaky_relu)(y)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Dense(512, activation=leaky_relu)(y)\n",
    "output = Dense(HYPER_PARAMS[\"window-height\"], activation=\"linear\")(y)\n",
    "\n",
    "# Construct the model\n",
    "model = Model(inputs=[mel_spectrogram_input, x_vector_input], outputs=output)\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=HYPER_PARAMS[\"learning-rate\"])\n",
    "loss_fn = MeanSquaredError()\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "class SamplesLoader:\n",
    "    def __init__(self, audio_samples, x_vectors, params):\n",
    "        self.audio_samples = audio_samples\n",
    "        self.x_vectors = x_vectors\n",
    "\n",
    "        self.params = params\n",
    "        self.augmenrations = {k: v for k, v in params.items() if k.startswith(\"augment\")}\n",
    "\n",
    "        self.clip_start_index = int(\n",
    "            self.params[\"min-frequency\"] / self.params[\"audio-rate\"] * self.params[\"fft-window\"] // 2\n",
    "        )\n",
    "        self.clip_end_index = self.clip_start_index + self.params[\"window-height\"]\n",
    "\n",
    "        self.active_batch = {\n",
    "            \"input\": tf.zeros(shape=(0, self.params[\"window-width\"], self.params[\"window-height\"])),\n",
    "            \"x-vector\": tf.zeros(shape=(0, self.params[\"x-vector-dim\"])),\n",
    "            \"output\": tf.zeros(shape=(0, self.params[\"window-height\"])),\n",
    "        }\n",
    "\n",
    "        self.active_samples_iter = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.active_samples_iter = self.audio_samples.sample(frac=1).iterrows()\n",
    "        return self\n",
    "\n",
    "    def _slice_batch(self):\n",
    "        if self.active_batch[\"input\"].shape[0] < self.params[\"batch-size\"]:\n",
    "            return None\n",
    "\n",
    "        new_batch = dict()\n",
    "        for key in self.active_batch.keys():\n",
    "            new_batch[key] = self.active_batch[key][: self.params[\"batch-size\"]]\n",
    "            self.active_batch[key] = self.active_batch[key][self.params[\"batch-size\"] :]\n",
    "\n",
    "        return new_batch[\"input\"], new_batch[\"x-vector\"], new_batch[\"output\"]\n",
    "\n",
    "    def _get_spectrogram(self, audio_tensor):\n",
    "        spectrogram = tfio.audio.spectrogram(\n",
    "            audio_tensor,\n",
    "            nfft=self.params[\"nfft\"],\n",
    "            window=self.params[\"fft-window\"],\n",
    "            stride=self.params[\"fft-stride\"],\n",
    "        )\n",
    "\n",
    "        # Slice away frequencies outside of the human voice range\n",
    "        sliced_tensor = tf.slice(\n",
    "            spectrogram,\n",
    "            [0, self.clip_start_index],\n",
    "            [spectrogram.shape[0], self.clip_end_index],\n",
    "        )\n",
    "        mean = tf.math.reduce_mean(sliced_tensor)\n",
    "        std_dev = tf.math.reduce_std(sliced_tensor)\n",
    "        final_tensor = (sliced_tensor - mean) / std_dev  # Normalize the tensor\n",
    "\n",
    "        return final_tensor\n",
    "\n",
    "    def _process_audio_tensor(self, audio_tensor):\n",
    "        final_tensor = self._get_spectrogram(audio_tensor)  # Get normalized tensor\n",
    "\n",
    "        # Calculate padding width\n",
    "        pad_width = (self.params[\"window-width\"] - 1) // 2\n",
    "\n",
    "        # Pad the input image on both sides along the width axis\n",
    "        padded_image = tf.pad(final_tensor, [[pad_width, pad_width], [0, 0]])\n",
    "\n",
    "        # Extract patches with a sliding window\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=tf.expand_dims(\n",
    "                tf.expand_dims(padded_image, -1), 0\n",
    "            ),  # Add a batch dimension to the input image\n",
    "            sizes=[\n",
    "                1,\n",
    "                self.params[\"window-width\"],\n",
    "                self.params[\"window-height\"],\n",
    "                1,\n",
    "            ],  # Patch size (1, w_w, h, 1)\n",
    "            strides=[\n",
    "                1,\n",
    "                1,\n",
    "                1,\n",
    "                1,\n",
    "            ],  # Stride (1, 1, 1, 1) for a sliding window with a step of 1\n",
    "            rates=[1, 1, 1, 1],  # Dilation rate (1, 1, 1, 1)\n",
    "            padding=\"VALID\",  # No padding is required as we already padded the input image\n",
    "        )\n",
    "\n",
    "        # Reshape the patches tensor to the desired output shape (w, w_w, h)\n",
    "        patches = tf.reshape(\n",
    "            patches, [final_tensor.shape[0], self.params[\"window-width\"], self.params[\"window-height\"]]\n",
    "        )\n",
    "        return patches\n",
    "    \n",
    "    # Augmentation functions should return input and ouput audio tensors\n",
    "    def _augment_raw(self, sample):\n",
    "        return sample, sample\n",
    "\n",
    "    def _augment_empty_noise(self, sample):\n",
    "        noise_level = tf.random.uniform(shape=(), minval=self.params[\"noise-min-level\"], maxval=self.params[\"noise-max-level\"], dtype=tf.float32)\n",
    "        noise_tensor = tf.random.normal(shape=tf.shape(sample), mean=0.0, stddev=noise_level, dtype=tf.float32)\n",
    "        noise_tensor *= random() * 2\n",
    "        return noise_tensor, noise_tensor\n",
    "\n",
    "    def _augment_noise(self, sample):\n",
    "        noise_level = tf.random.uniform(shape=(), minval=self.params[\"noise-min-level\"], maxval=self.params[\"noise-max-level\"], dtype=tf.float32)\n",
    "        noise_tensor = tf.random.normal(shape=tf.shape(sample), mean=0.0, stddev=noise_level, dtype=tf.float32)\n",
    "        return sample + noise_tensor, sample\n",
    "\n",
    "    def _augment_dialogue(self, sample):\n",
    "        speakers_count = tf.random.uniform(shape=(), minval=self.params[\"dialogue-min-speakers\"], maxval=self.params[\"dialogue-max-speakers\"], dtype=tf.float32)\n",
    "        input_tensor = tf.identity(sample)\n",
    "        input_length = sample.shape[0]\n",
    "\n",
    "        for _ in range(int(speakers_count)):\n",
    "            random_sample = self.audio_samples.sample(n=1).iloc[0]\n",
    "            random_audio = self._load_mp3_sample(random_sample)\n",
    "            random_audio_length = random_audio.shape[0]\n",
    "            \n",
    "            if random_audio_length > input_length:\n",
    "                start_index = np.random.randint(0, random_audio_length - input_length)\n",
    "                end_index = start_index + input_length\n",
    "                input_tensor += random_audio[start_index:end_index]\n",
    "            else:\n",
    "                left_padding = (input_length - random_audio_length) // 2\n",
    "                right_padding = input_length - random_audio_length - left_padding\n",
    "\n",
    "                padded_tensor = tf.pad(random_audio, paddings=[[left_padding, right_padding]])\n",
    "                input_tensor += padded_tensor\n",
    "\n",
    "\n",
    "        input_tensor /= speakers_count * tf.cast(self.params[\"dialogue-normalization-factor\"], dtype=tf.float32)\n",
    "        return input_tensor, sample\n",
    "\n",
    "    def _augment_dialogue_noise(self, sample):\n",
    "        dialogue_sample, _ = self._augment_dialogue(sample)\n",
    "        noise_sample, _ = self._augment_empty_noise(dialogue_sample)\n",
    "        return dialogue_sample, noise_sample\n",
    "\n",
    "\n",
    "    def _match_augmentation(self, sample, augmentation):\n",
    "        if augmentation == \"augment-raw\":\n",
    "            return self._augment_raw(sample)\n",
    "        elif augmentation == \"augment-empty-noise\":\n",
    "            return self._augment_empty_noise(sample)\n",
    "        elif augmentation == \"augment-noise\":\n",
    "            return self._augment_noise(sample)\n",
    "        elif augmentation == \"augment-dialogue\":\n",
    "            return self._augment_dialogue(sample)\n",
    "        elif augmentation == \"augment-dialogue-noise\":\n",
    "            return self._augment_dialogue_noise(sample)\n",
    "        \n",
    "    def _load_mp3_sample(self, sample):\n",
    "        sample_path = get_path(sample[\"path\"])\n",
    "        sample_binary = tf.io.read_file(sample_path)\n",
    "        audio = tfio.audio.decode_mp3(sample_binary)\n",
    "\n",
    "        return tf.squeeze(audio, axis=-1)\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            existing_batch = self._slice_batch()\n",
    "            if existing_batch is not None:\n",
    "                return existing_batch\n",
    "\n",
    "            iter_result = self.active_samples_iter.__next__()\n",
    "            if iter_result is None:\n",
    "                raise StopIteration  # TODO: Pad left over samples with zeros\n",
    "\n",
    "            sample_index, new_sample = iter_result\n",
    "            sample_x_vector = self.x_vectors[new_sample[\"client_id\"]]\n",
    "            if sample_x_vector is None:\n",
    "                continue\n",
    "\n",
    "            # Load audio sample\n",
    "            audio = self._load_mp3_sample(new_sample)\n",
    "\n",
    "            if audio.shape[0] < 1000:  # Skip to short samples\n",
    "                continue\n",
    "\n",
    "            # Crop audio beggining and end to remove silence and stop/start button clicks\n",
    "            raw_audio_tensor = audio[200:-250]\n",
    "            raw_audio_spectrogram = self._get_spectrogram(raw_audio_tensor)\n",
    "            tiled_x_vector = tf.tile(\n",
    "                tf.expand_dims(sample_x_vector, 0), (raw_audio_spectrogram.shape[0], 1)\n",
    "            )\n",
    "\n",
    "            modified_samples = []\n",
    "            for augmentation_name, possibility in self.augmenrations.items():\n",
    "                if possibility > random():\n",
    "                    new_sample = self._match_augmentation(raw_audio_tensor, augmentation_name)\n",
    "                    if new_sample is None:\n",
    "                        print(f\"WARNING: Match for {augmentation_name} augmentation returned None\")\n",
    "                        continue\n",
    "                    modified_samples.append(new_sample)\n",
    "\n",
    "            for input_sample, output_sample in modified_samples:\n",
    "                input_sample_patches = self._process_audio_tensor(input_sample)\n",
    "                output_spectrogram = self._get_spectrogram(output_sample)\n",
    "\n",
    "                self.active_batch[\"input\"] = tf.concat(\n",
    "                    [self.active_batch[\"input\"], input_sample_patches], axis=0\n",
    "                )\n",
    "                self.active_batch[\"x-vector\"] = tf.concat(\n",
    "                    [self.active_batch[\"x-vector\"], tiled_x_vector], axis=0\n",
    "                )\n",
    "                self.active_batch[\"output\"] = tf.concat(\n",
    "                    [self.active_batch[\"output\"], output_spectrogram], axis=0\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_start_index = int(raw_clips_meta.shape[0] * HYPER_PARAMS[\"learn-test-split\"])\n",
    "train_dataset = raw_clips_meta.iloc[:train_test_start_index]\n",
    "test_dataset = raw_clips_meta.iloc[train_test_start_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_generator(loader):\n",
    "    for batch_X_mel, batch_X_xvec, batch_y in loader:\n",
    "        yield (batch_X_mel, batch_X_xvec), batch_y\n",
    "\n",
    "\n",
    "train_loader = SamplesLoader(\n",
    "    train_dataset, speakers_xvectors, HYPER_PARAMS\n",
    ")\n",
    "validation_loader = SamplesLoader(\n",
    "    test_dataset, speakers_xvectors, HYPER_PARAMS\n",
    ")\n",
    "\n",
    "test_batch = next(iter(validation_loader))\n",
    "output_signature = (\n",
    "    tuple(\n",
    "        [\n",
    "            tf.TensorSpec.from_tensor(test_batch[0]),\n",
    "            tf.TensorSpec.from_tensor(test_batch[1]),\n",
    "        ]\n",
    "    ),\n",
    "    tf.TensorSpec.from_tensor(test_batch[2]),\n",
    ")\n",
    "\n",
    "train_data = tf.data.Dataset.from_generator(\n",
    "    lambda: samples_generator(train_loader),\n",
    "    output_signature=output_signature,\n",
    ")\n",
    "\n",
    "validation_data = tf.data.Dataset.from_generator(\n",
    "    lambda: samples_generator(validation_loader),\n",
    "    output_signature=output_signature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msllowre\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback, WandbModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/speach-separator/wandb/run-20230430_230128-vxmg1j3v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sllowre/speech-filter/runs/vxmg1j3v' target=\"_blank\">iconic-wood-12</a></strong> to <a href='https://wandb.ai/sllowre/speech-filter' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sllowre/speech-filter' target=\"_blank\">https://wandb.ai/sllowre/speech-filter</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sllowre/speech-filter/runs/vxmg1j3v' target=\"_blank\">https://wandb.ai/sllowre/speech-filter/runs/vxmg1j3v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 23:01:33.522059: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-30 23:01:33.665245: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-30 23:01:37.478479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-04-30 23:01:41.096166: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f80a2a5dfd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-30 23:01:41.096222: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2023-04-30 23:01:41.123360: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-04-30 23:01:41.449697: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-04-30 23:01:42.675336: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-04-30 23:01:42.800809: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.30GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6/Unknown - 12s 168ms/step - loss: 1.1674WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0116s vs `on_train_batch_end` time: 0.1315s). Check your callbacks.\n",
      "   2750/Unknown - 479s 170ms/step - loss: 0.9989"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 31\u001b[0m\n\u001b[1;32m      4\u001b[0m checkpoint_callback \u001b[39m=\u001b[39m WandbModelCheckpoint(\n\u001b[1;32m      5\u001b[0m     filepath\u001b[39m=\u001b[39mcheckpoint_path,\n\u001b[1;32m      6\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     save_freq\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m metrics_callback \u001b[39m=\u001b[39m WandbCallback(\n\u001b[1;32m     15\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     log_batch_frequency\u001b[39m=\u001b[39mHYPER_PARAMS[\u001b[39m\"\u001b[39m\u001b[39mlogs-batch-frequency\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     32\u001b[0m     train_data,\n\u001b[1;32m     33\u001b[0m     epochs\u001b[39m=\u001b[39;49mHYPER_PARAMS[\u001b[39m\"\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     34\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m     35\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     36\u001b[0m         metrics_callback,\n\u001b[1;32m     37\u001b[0m         checkpoint_callback,\n\u001b[1;32m     38\u001b[0m     ],\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m run\u001b[39m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/engine/training.py:1691\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1689\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[1;32m   1690\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1691\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1692\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1693\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 475\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    321\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    327\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    343\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 345\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    348\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    392\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 393\u001b[0m     hook(batch, logs)\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[1;32m    396\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1093\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/callbacks.py:1170\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1168\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m-> 1170\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprogbar\u001b[39m.\u001b[39;49mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseen, \u001b[39mlist\u001b[39;49m(logs\u001b[39m.\u001b[39;49mitems()), finalize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/utils/generic_utils.py:296\u001b[0m, in \u001b[0;36mProgbar.update\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    293\u001b[0m         info \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m info\n\u001b[0;32m--> 296\u001b[0m     io_utils\u001b[39m.\u001b[39;49mprint_msg(message, line_break\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    297\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/utils/io_utils.py:75\u001b[0m, in \u001b[0;36mprint_msg\u001b[0;34m(message, line_break)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_msg\u001b[39m(message, line_break\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     74\u001b[0m     \u001b[39m\"\"\"Print the message to absl logging or stdout.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mif\u001b[39;00m is_interactive_logging_enabled():\n\u001b[1;32m     76\u001b[0m         \u001b[39mif\u001b[39;00m line_break:\n\u001b[1;32m     77\u001b[0m             sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mwrite(message \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/utils/io_utils.py:55\u001b[0m, in \u001b[0;36mis_interactive_logging_enabled\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[39m\"\"\"Turn off interactive logging.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[39m    When interactive logging is disabled, Keras sends logs to `absl.logging`.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m    This is the best option when using Keras in a non-interactive\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m    way, such as running a training or inference job on a server.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     INTERACTIVE_LOGGING\u001b[39m.\u001b[39menable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mkeras.utils.is_interactive_logging_enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_interactive_logging_enabled\u001b[39m():\n\u001b[1;32m     57\u001b[0m     \u001b[39m\"\"\"Check if interactive logging is enabled.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[39m    To switch between writing logs to stdout and `absl.logging`, you may use\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m      Boolean (True if interactive logging is enabled and False otherwise).\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[39m# Use `getattr` in case `INTERACTIVE_LOGGING`\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[39m# does not have the `enable` attribute.\u001b[39;00m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1363\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1664\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_is_thread_alive.py:9\u001b[0m, in \u001b[0;36mis_thread_alive\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      6\u001b[0m _temp \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mThread()\n\u001b[1;32m      7\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(_temp, \u001b[39m'\u001b[39m\u001b[39m_is_stopped\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# Python 3.x has this\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mis_thread_alive\u001b[39m(t):\n\u001b[1;32m     10\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mnot\u001b[39;00m t\u001b[39m.\u001b[39m_is_stopped\n\u001b[1;32m     12\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(_temp, \u001b[39m'\u001b[39m\u001b[39m_Thread__stopped\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# Python 2.x has this\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run = wandb.init(config=HYPER_PARAMS, project=\"speech-filter\")\n",
    "checkpoint_path = wandb.run.dir + \"/model_checkpoint.hdf5\"\n",
    "\n",
    "checkpoint_callback = WandbModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\",\n",
    ")\n",
    "\n",
    "metrics_callback = WandbCallback(\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    save_model=True,\n",
    "    save_graph=True,\n",
    "    save_weights_only=False,\n",
    "    log_weights=True,\n",
    "    log_gradients=True,\n",
    "    training_data=train_data,\n",
    "    validation_data=validation_data,\n",
    "    predictions=64,\n",
    "    input_type=\"images\",\n",
    "    output_type=\"image\",\n",
    "    log_batch_frequency=HYPER_PARAMS[\"logs-batch-frequency\"],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_data,\n",
    "    epochs=HYPER_PARAMS[\"epochs\"],\n",
    "    validation_data=validation_data,\n",
    "    callbacks=[\n",
    "        metrics_callback,\n",
    "        checkpoint_callback,\n",
    "    ],\n",
    ")\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
