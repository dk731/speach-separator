{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 20:49:25.150050: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-30 20:49:31.501348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-04-30 20:49:34.466248: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-30 20:49:36.770243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-30 20:49:36.770313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import kaldiio\n",
    "\n",
    "from scipy.signal import welch\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "tf.config.list_physical_devices(\"GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = os.getenv(\"PROJECT_ROOT\")\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "CLIPS_PATH = os.getenv(\"CLIPS_PATH\")\n",
    "\n",
    "VALIDATED_LIST_PATH = os.path.join(os.getenv(\"CLIPS_META_PATH\"), \"validated.tsv\")\n",
    "XVECTOR_RESULT_PATH = os.getenv(\"XVECTOR_RESULT_PATH\")\n",
    "XVECTOR_SCP_PATH = os.path.join(XVECTOR_RESULT_PATH, \"xvector.scp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_xvectors = kaldiio.load_scp(XVECTOR_SCP_PATH)\n",
    "valid_speakers = set(speakers_xvectors.keys())\n",
    "\n",
    "raw_clips_meta = pd.read_table(VALIDATED_LIST_PATH)\n",
    "raw_clips_meta = raw_clips_meta[raw_clips_meta[\"client_id\"].isin(valid_speakers)]\n",
    "\n",
    "\n",
    "def get_path(row):\n",
    "    return f\"{CLIPS_PATH}/{row}.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    Concatenate,\n",
    "    LeakyReLU,\n",
    "    BatchNormalization,\n",
    "    Flatten,\n",
    "    AveragePooling2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " spectrogram_input (InputLayer)  [(None, 65, 682, 1)  0          []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 65, 682, 32)  320         ['spectrogram_input[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 65, 682, 32)  128        ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " average_pooling2d_5 (AveragePo  (None, 32, 341, 32)  0          ['batch_normalization_5[0][0]']  \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 341, 64)  18496       ['average_pooling2d_5[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 341, 64)  256        ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " average_pooling2d_6 (AveragePo  (None, 16, 170, 64)  0          ['batch_normalization_6[0][0]']  \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 16, 170, 128  73856       ['average_pooling2d_6[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 16, 170, 128  512        ['conv2d_7[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " average_pooling2d_7 (AveragePo  (None, 8, 85, 128)  0           ['batch_normalization_7[0][0]']  \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 87040)        0           ['average_pooling2d_7[0][0]']    \n",
      "                                                                                                  \n",
      " x_vector_input (InputLayer)    [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 87552)        0           ['flatten_1[0][0]',              \n",
      "                                                                  'x_vector_input[0][0]']         \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 192)          16810176    ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 192)          0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 256)          49408       ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 256)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 256)          65792       ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 256)          0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 512)          131584      ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 682)          349866      ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,500,394\n",
      "Trainable params: 17,499,946\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "HYPER_PARAMS = {\n",
    "    # Model parameters\n",
    "    \"window-width\": 65,\n",
    "    \"x-vector-dim\": 512,\n",
    "    # Training parameters\n",
    "    \"batch-size\": 256,\n",
    "    \"epochs\": 10,\n",
    "    \"learning-rate\": 0.000005,\n",
    "    \"learn-test-split\": 0.8,\n",
    "    \"logs-batch-frequency\": 25,\n",
    "    # FFT parameters\n",
    "    \"nfft\": 8192,\n",
    "    \"fft-window\": 4096,\n",
    "    \"fft-stride\": 512,\n",
    "    # General\n",
    "    \"min-frequency\": 0,\n",
    "    \"max-frequency\": 10e3,\n",
    "    \"audio-rate\": 48e3,\n",
    "}\n",
    "\n",
    "HYPER_PARAMS[\"window-height\"] = int(\n",
    "    (HYPER_PARAMS[\"max-frequency\"] - HYPER_PARAMS[\"min-frequency\"])\n",
    "    / HYPER_PARAMS[\"audio-rate\"]\n",
    "    * (HYPER_PARAMS[\"nfft\"] // 2 + 1)\n",
    ")\n",
    "\n",
    "# Model parameters\n",
    "mel_spectrogram_shape = (\n",
    "    HYPER_PARAMS[\"window-width\"],\n",
    "    HYPER_PARAMS[\"window-height\"],\n",
    "    1,\n",
    ")  # Replace window_size and num_mel_bands with your values\n",
    "\n",
    "# Leaky ReLU activation function\n",
    "leaky_relu = LeakyReLU(alpha=0.2)\n",
    "\n",
    "# Mel-spectrogram input\n",
    "mel_spectrogram_input = Input(shape=mel_spectrogram_shape, name=\"spectrogram_input\")\n",
    "\n",
    "# Convolutional layers\n",
    "x = Conv2D(16, (3, 3), padding=\"same\", activation=leaky_relu)(mel_spectrogram_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Convolutional layers\n",
    "x = Conv2D(32, (3, 3), padding=\"same\", activation=leaky_relu)(mel_spectrogram_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), padding=\"same\", activation=leaky_relu)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(128, (3, 3), padding=\"same\", activation=leaky_relu)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "# X-vector input\n",
    "x_vector_input = Input(shape=(HYPER_PARAMS[\"x-vector-dim\"],), name=\"x_vector_input\")\n",
    "\n",
    "# Concatenate flattened CNN output with x-vector input\n",
    "combined_input = Concatenate()([x, x_vector_input])\n",
    "\n",
    "# Dense layers\n",
    "y = Dense(128, activation=leaky_relu)(combined_input)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Dense(192, activation=leaky_relu)(combined_input)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Dense(256, activation=leaky_relu)(y)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Dense(256, activation=leaky_relu)(y)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Dense(512, activation=leaky_relu)(y)\n",
    "output = Dense(HYPER_PARAMS[\"window-height\"], activation=\"linear\")(y)\n",
    "\n",
    "# Construct the model\n",
    "model = Model(inputs=[mel_spectrogram_input, x_vector_input], outputs=output)\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=HYPER_PARAMS[\"learning-rate\"])\n",
    "loss_fn = MeanSquaredError()\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "\n",
    "class SamplesLoader:\n",
    "    def __init__(self, audio_samples, x_vectors, modifications, params):\n",
    "        self.audio_samples = audio_samples\n",
    "        self.x_vectors = x_vectors\n",
    "\n",
    "        self.batch_size = params[\"batch-size\"]\n",
    "        self.split = params[\"learn-test-split\"]\n",
    "        self.nfft = params[\"nfft\"]\n",
    "        self.fft_window = params[\"fft-window\"]\n",
    "        self.fft_stride = params[\"fft-stride\"]\n",
    "\n",
    "        self.min_frequency = params[\"min-frequency\"]\n",
    "        self.max_frequency = params[\"max-frequency\"]\n",
    "        self.audio_rate = params[\"audio-rate\"]\n",
    "\n",
    "        self.window_width = params[\"window-width\"]\n",
    "        self.window_height = params[\"window-height\"]\n",
    "\n",
    "        self.min_frequency = params[\"min-frequency\"]\n",
    "        self.max_frequency = params[\"max-frequency\"]\n",
    "\n",
    "        self.x_vector_size = params[\"x-vector-dim\"]\n",
    "\n",
    "        self.clip_start_index = int(\n",
    "            self.min_frequency / self.audio_rate * self.fft_window // 2\n",
    "        )\n",
    "        self.clip_end_index = self.clip_start_index + self.window_height\n",
    "\n",
    "        # Modification are dict where key is possibility of modification\n",
    "        # and value is callback that accepts raw sample and returns modified sample\n",
    "        self.modifications = modifications\n",
    "\n",
    "        self.active_batch = {\n",
    "            \"input\": tf.zeros(shape=(0, self.window_width, self.window_height)),\n",
    "            \"x-vector\": tf.zeros(shape=(0, self.x_vector_size)),\n",
    "            \"output\": tf.zeros(shape=(0, self.window_height)),\n",
    "        }\n",
    "\n",
    "        self.active_samples_iter = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.active_samples_iter = self.audio_samples.sample(frac=1).iterrows()\n",
    "        return self\n",
    "\n",
    "    def _slice_batch(self):\n",
    "        if self.active_batch[\"input\"].shape[0] < self.batch_size:\n",
    "            return None\n",
    "\n",
    "        new_batch = dict()\n",
    "        for key in self.active_batch.keys():\n",
    "            new_batch[key] = self.active_batch[key][: self.batch_size]\n",
    "            self.active_batch[key] = self.active_batch[key][self.batch_size :]\n",
    "\n",
    "        return new_batch[\"input\"], new_batch[\"x-vector\"], new_batch[\"output\"]\n",
    "\n",
    "    def _get_spectrogram(self, audio_tensor):\n",
    "        spectrogram = tfio.audio.spectrogram(\n",
    "            audio_tensor,\n",
    "            nfft=self.nfft,\n",
    "            window=self.fft_window,\n",
    "            stride=self.fft_stride,\n",
    "        )\n",
    "\n",
    "        # Slice away frequencies outside of the human voice range\n",
    "        sliced_tensor = tf.slice(\n",
    "            spectrogram,\n",
    "            [0, self.clip_start_index],\n",
    "            [spectrogram.shape[0], self.clip_end_index],\n",
    "        )\n",
    "        mean = tf.math.reduce_mean(sliced_tensor)\n",
    "        std_dev = tf.math.reduce_std(sliced_tensor)\n",
    "        final_tensor = (sliced_tensor - mean) / std_dev  # Normalize the tensor\n",
    "\n",
    "        return final_tensor\n",
    "\n",
    "    def _process_audio_tensor(self, audio_tensor):\n",
    "        final_tensor = self._get_spectrogram(audio_tensor)  # Get normalized tensor\n",
    "\n",
    "        # Calculate padding width\n",
    "        pad_width = (self.window_width - 1) // 2\n",
    "\n",
    "        # Pad the input image on both sides along the width axis\n",
    "        padded_image = tf.pad(final_tensor, [[pad_width, pad_width], [0, 0]])\n",
    "\n",
    "        # Extract patches with a sliding window\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=tf.expand_dims(\n",
    "                tf.expand_dims(padded_image, -1), 0\n",
    "            ),  # Add a batch dimension to the input image\n",
    "            sizes=[\n",
    "                1,\n",
    "                self.window_width,\n",
    "                self.window_height,\n",
    "                1,\n",
    "            ],  # Patch size (1, w_w, h, 1)\n",
    "            strides=[\n",
    "                1,\n",
    "                1,\n",
    "                1,\n",
    "                1,\n",
    "            ],  # Stride (1, 1, 1, 1) for a sliding window with a step of 1\n",
    "            rates=[1, 1, 1, 1],  # Dilation rate (1, 1, 1, 1)\n",
    "            padding=\"VALID\",  # No padding is required as we already padded the input image\n",
    "        )\n",
    "\n",
    "        # Reshape the patches tensor to the desired output shape (w, w_w, h)\n",
    "        patches = tf.reshape(\n",
    "            patches, [final_tensor.shape[0], self.window_width, self.window_height]\n",
    "        )\n",
    "        return patches\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            existing_batch = self._slice_batch()\n",
    "            if existing_batch is not None:\n",
    "                return existing_batch\n",
    "\n",
    "            iter_result = self.active_samples_iter.__next__()\n",
    "            if iter_result is None:\n",
    "                raise StopIteration  # TODO: Pad left over samples with zeros\n",
    "\n",
    "            sample_index, new_sample = iter_result\n",
    "            sample_x_vector = self.x_vectors[new_sample[\"client_id\"]]\n",
    "            if sample_x_vector is None:\n",
    "                continue\n",
    "\n",
    "            # Load audio sample\n",
    "            audio = tfio.audio.AudioIOTensor(get_path(new_sample[\"path\"]))\n",
    "            if audio.shape[0] < 1000:  # Skip to short samples\n",
    "                continue\n",
    "\n",
    "            # Crop audio beggining and end to remove silence and stop/start button clicks\n",
    "            raw_audio_tensor = tf.squeeze(audio[300:-350], axis=[-1])\n",
    "            raw_audio_spectrogram = self._get_spectrogram(raw_audio_tensor)\n",
    "            tiled_x_vector = tf.tile(\n",
    "                tf.expand_dims(sample_x_vector, 0), (raw_audio_spectrogram.shape[0], 1)\n",
    "            )\n",
    "\n",
    "            modified_samples = []\n",
    "            for possibility, callback in self.modifications.items():\n",
    "                if possibility > random():\n",
    "                    modified_samples.append(callback(raw_audio_tensor))\n",
    "\n",
    "            for modified_sample in modified_samples:\n",
    "                sample_patches = self._process_audio_tensor(modified_sample)\n",
    "\n",
    "                self.active_batch[\"input\"] = tf.concat(\n",
    "                    [self.active_batch[\"input\"], sample_patches], axis=0\n",
    "                )\n",
    "                self.active_batch[\"x-vector\"] = tf.concat(\n",
    "                    [self.active_batch[\"x-vector\"], tiled_x_vector], axis=0\n",
    "                )\n",
    "                self.active_batch[\"output\"] = tf.concat(\n",
    "                    [self.active_batch[\"output\"], raw_audio_spectrogram], axis=0\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_start_index = int(raw_clips_meta.shape[0] * HYPER_PARAMS[\"learn-test-split\"])\n",
    "train_dataset = raw_clips_meta.iloc[:train_test_start_index]\n",
    "test_dataset = raw_clips_meta.iloc[train_test_start_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_generator(loader):\n",
    "    for batch_X_mel, batch_X_xvec, batch_y in loader:\n",
    "        yield (batch_X_mel, batch_X_xvec), batch_y\n",
    "\n",
    "\n",
    "train_loader = SamplesLoader(\n",
    "    train_dataset, speakers_xvectors, {1: lambda sample: sample}, HYPER_PARAMS\n",
    ")\n",
    "validation_loader = SamplesLoader(\n",
    "    test_dataset, speakers_xvectors, {1: lambda sample: sample}, HYPER_PARAMS\n",
    ")\n",
    "\n",
    "test_batch = next(iter(validation_loader))\n",
    "output_signature = (\n",
    "    tuple(\n",
    "        [\n",
    "            tf.TensorSpec.from_tensor(test_batch[0]),\n",
    "            tf.TensorSpec.from_tensor(test_batch[1]),\n",
    "        ]\n",
    "    ),\n",
    "    tf.TensorSpec.from_tensor(test_batch[2]),\n",
    ")\n",
    "\n",
    "train_data = tf.data.Dataset.from_generator(\n",
    "    lambda: samples_generator(train_loader),\n",
    "    output_signature=output_signature,\n",
    ")\n",
    "\n",
    "validation_data = tf.data.Dataset.from_generator(\n",
    "    lambda: samples_generator(validation_loader),\n",
    "    output_signature=output_signature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback, WandbModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2uxi6x72) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90995eea618e47c1bd40c290c6b3b1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dark-dawn-7</strong> at: <a href='https://wandb.ai/sllowre/speech-filter/runs/2uxi6x72' target=\"_blank\">https://wandb.ai/sllowre/speech-filter/runs/2uxi6x72</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230430_205006-2uxi6x72/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2uxi6x72). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52f845973ea439e8c592e6674290af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669755383342515, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/speach-separator/wandb/run-20230430_205314-kh3vfgpd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sllowre/speech-filter/runs/kh3vfgpd' target=\"_blank\">cool-snow-8</a></strong> to <a href='https://wandb.ai/sllowre/speech-filter' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sllowre/speech-filter' target=\"_blank\">https://wandb.ai/sllowre/speech-filter</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sllowre/speech-filter/runs/kh3vfgpd' target=\"_blank\">https://wandb.ai/sllowre/speech-filter/runs/kh3vfgpd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 20:53:28.222485: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-30 20:53:28.350331: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-30 20:53:43.280703: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.53GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-04-30 20:53:43.319914: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.53GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-04-30 20:53:43.789936: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.77GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-04-30 20:53:43.790042: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.77GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-04-30 20:53:43.860959: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f507766ef80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-30 20:53:43.861014: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2023-04-30 20:53:45.990438: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-04-30 20:53:51.688878: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-04-30 20:54:01.249154: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.36GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-04-30 20:54:01.557573: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.34GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-04-30 20:54:02.170851: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.71GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-04-30 20:54:02.521627: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.67GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6/Unknown - 39s 397ms/step - loss: 1.0393WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0828s vs `on_train_batch_end` time: 0.2634s). Check your callbacks.\n",
      "     24/Unknown - 46s 394ms/step - loss: 1.0323"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 20:54:24.416712: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.58GiB (rounded to 1702272000)requested by op gradient_tape/model_2/average_pooling2d_5/AvgPoolGrad\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-04-30 20:54:24.416816: I tensorflow/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2023-04-30 20:54:24.416832: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 120, Chunks in use: 120. 30.0KiB allocated for chunks. 30.0KiB in use in bin. 9.7KiB client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416838: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 33, Chunks in use: 32. 18.8KiB allocated for chunks. 18.0KiB in use in bin. 17.6KiB client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416842: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 21, Chunks in use: 21. 23.5KiB allocated for chunks. 23.5KiB in use in bin. 22.4KiB client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416845: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 11, Chunks in use: 11. 28.0KiB allocated for chunks. 28.0KiB in use in bin. 27.3KiB client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416849: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 1, Chunks in use: 1. 4.5KiB allocated for chunks. 4.5KiB in use in bin. 2.7KiB client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416853: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416858: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416861: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416865: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 7, Chunks in use: 7. 549.2KiB allocated for chunks. 549.2KiB in use in bin. 504.0KiB client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416869: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 6, Chunks in use: 6. 1.18MiB allocated for chunks. 1.18MiB in use in bin. 1.12MiB client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416873: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 12, Chunks in use: 12. 3.55MiB allocated for chunks. 3.55MiB in use in bin. 3.19MiB client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416876: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 9, Chunks in use: 8. 5.11MiB allocated for chunks. 4.60MiB in use in bin. 4.28MiB client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416880: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 13, Chunks in use: 13. 18.24MiB allocated for chunks. 18.24MiB in use in bin. 16.64MiB client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416884: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 4, Chunks in use: 2. 10.69MiB allocated for chunks. 4.21MiB in use in bin. 2.66MiB client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416887: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416890: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416894: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416898: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 7, Chunks in use: 5. 345.17MiB allocated for chunks. 256.18MiB in use in bin. 236.38MiB client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416902: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 10, Chunks in use: 10. 793.91MiB allocated for chunks. 793.91MiB in use in bin. 679.79MiB client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416906: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416910: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 7, Chunks in use: 4. 8.18GiB allocated for chunks. 5.16GiB in use in bin. 5.15GiB client-requested in use in bin.\n",
      "2023-04-30 20:54:24.416914: I tensorflow/tsl/framework/bfc_allocator.cc:1062] Bin for 1.58GiB was 256.00MiB, Chunk State: \n",
      "2023-04-30 20:54:24.416923: I tensorflow/tsl/framework/bfc_allocator.cc:1068]   Size: 256.20MiB | Requested Size: 1.88MiB | in_use: 0 | bin_num: 20, prev:   Size: 1.41MiB | Requested Size: 1.41MiB | in_use: 1 | bin_num: -1, next:   Size: 1.58GiB | Requested Size: 1.58GiB | in_use: 1 | bin_num: -1\n",
      "2023-04-30 20:54:24.416928: I tensorflow/tsl/framework/bfc_allocator.cc:1068]   Size: 1.23GiB | Requested Size: 1.45MiB | in_use: 0 | bin_num: 20, prev:   Size: 71.36MiB | Requested Size: 71.36MiB | in_use: 1 | bin_num: -1\n",
      "2023-04-30 20:54:24.416933: I tensorflow/tsl/framework/bfc_allocator.cc:1068]   Size: 1.54GiB | Requested Size: 799.22MiB | in_use: 0 | bin_num: 20, prev:   Size: 1.58GiB | Requested Size: 1.58GiB | in_use: 1 | bin_num: -1, next:   Size: 413.73MiB | Requested Size: 399.61MiB | in_use: 1 | bin_num: -1\n",
      "2023-04-30 20:54:24.416935: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 10018095104\n",
      "2023-04-30 20:54:24.416982: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508600000 of size 1280 next 1\n",
      "2023-04-30 20:54:24.416991: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508600500 of size 256 next 2\n",
      "2023-04-30 20:54:24.416994: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508600600 of size 256 next 3\n",
      "2023-04-30 20:54:24.416996: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508600700 of size 256 next 5\n",
      "2023-04-30 20:54:24.416999: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508600800 of size 256 next 6\n",
      "2023-04-30 20:54:24.417002: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508600900 of size 256 next 4\n",
      "2023-04-30 20:54:24.417004: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508600a00 of size 256 next 7\n",
      "2023-04-30 20:54:24.417007: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508600b00 of size 256 next 10\n",
      "2023-04-30 20:54:24.417009: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508600c00 of size 256 next 11\n",
      "2023-04-30 20:54:24.417012: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508600d00 of size 256 next 12\n",
      "2023-04-30 20:54:24.417014: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508600e00 of size 256 next 13\n",
      "2023-04-30 20:54:24.417017: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508600f00 of size 256 next 8\n",
      "2023-04-30 20:54:24.417020: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508601000 of size 768 next 9\n",
      "2023-04-30 20:54:24.417023: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508601300 of size 256 next 15\n",
      "2023-04-30 20:54:24.417025: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508601400 of size 256 next 16\n",
      "2023-04-30 20:54:24.417028: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508601500 of size 256 next 14\n",
      "2023-04-30 20:54:24.417031: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508601600 of size 256 next 17\n",
      "2023-04-30 20:54:24.417033: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508601700 of size 256 next 20\n",
      "2023-04-30 20:54:24.417036: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508601800 of size 256 next 21\n",
      "2023-04-30 20:54:24.417038: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508601900 of size 256 next 22\n",
      "2023-04-30 20:54:24.417041: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508601a00 of size 256 next 25\n",
      "2023-04-30 20:54:24.417044: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508601b00 of size 256 next 23\n",
      "2023-04-30 20:54:24.417047: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508601c00 of size 256 next 24\n",
      "2023-04-30 20:54:24.417049: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508601d00 of size 256 next 28\n",
      "2023-04-30 20:54:24.417052: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508601e00 of size 256 next 29\n",
      "2023-04-30 20:54:24.417054: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508601f00 of size 256 next 18\n",
      "2023-04-30 20:54:24.417057: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508602000 of size 1280 next 19\n",
      "2023-04-30 20:54:24.417060: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508602500 of size 256 next 30\n",
      "2023-04-30 20:54:24.417062: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508602600 of size 512 next 33\n",
      "2023-04-30 20:54:24.417065: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508602800 of size 512 next 31\n",
      "2023-04-30 20:54:24.417068: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508602a00 of size 512 next 32\n",
      "2023-04-30 20:54:24.417071: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508602c00 of size 512 next 36\n",
      "2023-04-30 20:54:24.417073: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508602e00 of size 512 next 37\n",
      "2023-04-30 20:54:24.417076: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508603000 of size 256 next 38\n",
      "2023-04-30 20:54:24.417078: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508603100 of size 256 next 39\n",
      "2023-04-30 20:54:24.417081: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508603200 of size 512 next 42\n",
      "2023-04-30 20:54:24.417083: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508603400 of size 256 next 40\n",
      "2023-04-30 20:54:24.417086: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508603500 of size 256 next 41\n",
      "2023-04-30 20:54:24.417089: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508603600 of size 768 next 45\n",
      "2023-04-30 20:54:24.417091: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508603900 of size 256 next 46\n",
      "2023-04-30 20:54:24.417094: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508603a00 of size 256 next 49\n",
      "2023-04-30 20:54:24.417096: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508603b00 of size 1024 next 52\n",
      "2023-04-30 20:54:24.417099: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508603f00 of size 256 next 50\n",
      "2023-04-30 20:54:24.417102: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508604000 of size 256 next 51\n",
      "2023-04-30 20:54:24.417104: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508604100 of size 1024 next 53\n",
      "2023-04-30 20:54:24.417107: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508604500 of size 256 next 55\n",
      "2023-04-30 20:54:24.417110: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508604600 of size 256 next 58\n",
      "2023-04-30 20:54:24.417112: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508604700 of size 2048 next 61\n",
      "2023-04-30 20:54:24.417115: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508604f00 of size 256 next 59\n",
      "2023-04-30 20:54:24.417118: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508605000 of size 256 next 60\n",
      "2023-04-30 20:54:24.417120: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508605100 of size 3584 next 66\n",
      "2023-04-30 20:54:24.417124: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508605f00 of size 256 next 64\n",
      "2023-04-30 20:54:24.417126: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508606000 of size 256 next 65\n",
      "2023-04-30 20:54:24.417129: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508606100 of size 256 next 69\n",
      "2023-04-30 20:54:24.417131: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508606200 of size 256 next 70\n",
      "2023-04-30 20:54:24.417134: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508606300 of size 256 next 71\n",
      "2023-04-30 20:54:24.417137: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508606400 of size 256 next 72\n",
      "2023-04-30 20:54:24.417140: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508606500 of size 256 next 73\n",
      "2023-04-30 20:54:24.417142: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508606600 of size 256 next 74\n",
      "2023-04-30 20:54:24.417145: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508606700 of size 256 next 76\n",
      "2023-04-30 20:54:24.417147: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508606800 of size 256 next 75\n",
      "2023-04-30 20:54:24.417150: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508606900 of size 256 next 89\n",
      "2023-04-30 20:54:24.417152: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508606a00 of size 256 next 79\n",
      "2023-04-30 20:54:24.417155: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508606b00 of size 1280 next 77\n",
      "2023-04-30 20:54:24.417158: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508607000 of size 1280 next 84\n",
      "2023-04-30 20:54:24.417160: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508607500 of size 256 next 85\n",
      "2023-04-30 20:54:24.417163: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508607600 of size 256 next 78\n",
      "2023-04-30 20:54:24.417165: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508607700 of size 256 next 80\n",
      "2023-04-30 20:54:24.417168: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508607800 of size 256 next 87\n",
      "2023-04-30 20:54:24.417170: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508607900 of size 256 next 90\n",
      "2023-04-30 20:54:24.417173: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508607a00 of size 256 next 92\n",
      "2023-04-30 20:54:24.417176: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508607b00 of size 256 next 94\n",
      "2023-04-30 20:54:24.417178: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508607c00 of size 256 next 95\n",
      "2023-04-30 20:54:24.417181: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508607d00 of size 256 next 96\n",
      "2023-04-30 20:54:24.417183: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508607e00 of size 256 next 97\n",
      "2023-04-30 20:54:24.417186: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508607f00 of size 256 next 98\n",
      "2023-04-30 20:54:24.417188: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508608000 of size 256 next 99\n",
      "2023-04-30 20:54:24.417191: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508608100 of size 512 next 101\n",
      "2023-04-30 20:54:24.417193: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508608300 of size 512 next 102\n",
      "2023-04-30 20:54:24.417196: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508608500 of size 512 next 103\n",
      "2023-04-30 20:54:24.417199: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508608700 of size 512 next 104\n",
      "2023-04-30 20:54:24.417201: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508608900 of size 512 next 105\n",
      "2023-04-30 20:54:24.417204: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508608b00 of size 512 next 106\n",
      "2023-04-30 20:54:24.417206: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508608d00 of size 768 next 109\n",
      "2023-04-30 20:54:24.417209: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508609000 of size 768 next 110\n",
      "2023-04-30 20:54:24.417211: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508609300 of size 1024 next 113\n",
      "2023-04-30 20:54:24.417214: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508609700 of size 1024 next 114\n",
      "2023-04-30 20:54:24.417217: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508609b00 of size 1024 next 117\n",
      "2023-04-30 20:54:24.417219: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508609f00 of size 1024 next 118\n",
      "2023-04-30 20:54:24.417222: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860a300 of size 2048 next 121\n",
      "2023-04-30 20:54:24.417225: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860ab00 of size 2048 next 122\n",
      "2023-04-30 20:54:24.417227: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860b300 of size 3584 next 124\n",
      "2023-04-30 20:54:24.417230: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860c100 of size 3584 next 125\n",
      "2023-04-30 20:54:24.417233: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860cf00 of size 256 next 126\n",
      "2023-04-30 20:54:24.417235: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860d000 of size 1536 next 127\n",
      "2023-04-30 20:54:24.417238: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860d600 of size 256 next 128\n",
      "2023-04-30 20:54:24.417240: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860d700 of size 256 next 129\n",
      "2023-04-30 20:54:24.417243: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860d800 of size 256 next 130\n",
      "2023-04-30 20:54:24.417246: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860d900 of size 256 next 131\n",
      "2023-04-30 20:54:24.417248: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860da00 of size 256 next 132\n",
      "2023-04-30 20:54:24.417251: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860db00 of size 256 next 133\n",
      "2023-04-30 20:54:24.417253: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860dc00 of size 256 next 138\n",
      "2023-04-30 20:54:24.417256: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860dd00 of size 256 next 141\n",
      "2023-04-30 20:54:24.417258: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860de00 of size 256 next 136\n",
      "2023-04-30 20:54:24.417261: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860df00 of size 256 next 137\n",
      "2023-04-30 20:54:24.417263: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860e000 of size 256 next 142\n",
      "2023-04-30 20:54:24.417266: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860e100 of size 256 next 139\n",
      "2023-04-30 20:54:24.417269: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860e200 of size 256 next 135\n",
      "2023-04-30 20:54:24.417271: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860e300 of size 256 next 148\n",
      "2023-04-30 20:54:24.417274: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860e400 of size 256 next 150\n",
      "2023-04-30 20:54:24.417276: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860e500 of size 768 next 140\n",
      "2023-04-30 20:54:24.417279: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860e800 of size 256 next 154\n",
      "2023-04-30 20:54:24.417281: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860e900 of size 256 next 157\n",
      "2023-04-30 20:54:24.417284: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860ea00 of size 256 next 155\n",
      "2023-04-30 20:54:24.417287: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860eb00 of size 256 next 156\n",
      "2023-04-30 20:54:24.417289: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860ec00 of size 256 next 270\n",
      "2023-04-30 20:54:24.417292: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860ed00 of size 256 next 160\n",
      "2023-04-30 20:54:24.417294: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860ee00 of size 512 next 163\n",
      "2023-04-30 20:54:24.417297: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860f000 of size 512 next 153\n",
      "2023-04-30 20:54:24.417299: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860f200 of size 1280 next 151\n",
      "2023-04-30 20:54:24.417302: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860f700 of size 512 next 161\n",
      "2023-04-30 20:54:24.417305: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860f900 of size 256 next 273\n",
      "2023-04-30 20:54:24.417307: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860fa00 of size 256 next 257\n",
      "2023-04-30 20:54:24.417310: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860fb00 of size 256 next 247\n",
      "2023-04-30 20:54:24.417313: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860fc00 of size 256 next 164\n",
      "2023-04-30 20:54:24.417315: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860fd00 of size 256 next 166\n",
      "2023-04-30 20:54:24.417318: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860fe00 of size 256 next 167\n",
      "2023-04-30 20:54:24.417320: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50860ff00 of size 512 next 170\n",
      "2023-04-30 20:54:24.417323: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508610100 of size 256 next 168\n",
      "2023-04-30 20:54:24.417325: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508610200 of size 256 next 169\n",
      "2023-04-30 20:54:24.417328: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508610300 of size 768 next 173\n",
      "2023-04-30 20:54:24.417331: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508610600 of size 1024 next 177\n",
      "2023-04-30 20:54:24.417333: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508610a00 of size 1024 next 174\n",
      "2023-04-30 20:54:24.417336: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508610e00 of size 2048 next 182\n",
      "2023-04-30 20:54:24.417338: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508611600 of size 256 next 176\n",
      "2023-04-30 20:54:24.417341: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508611700 of size 256 next 181\n",
      "2023-04-30 20:54:24.417344: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508611800 of size 4608 next 81\n",
      "2023-04-30 20:54:24.417346: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508612a00 of size 256 next 82\n",
      "2023-04-30 20:54:24.417349: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508612b00 of size 80384 next 27\n",
      "2023-04-30 20:54:24.417352: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508626500 of size 73728 next 26\n",
      "2023-04-30 20:54:24.417355: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508638500 of size 393216 next 54\n",
      "2023-04-30 20:54:24.417358: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508698500 of size 196608 next 35\n",
      "2023-04-30 20:54:24.417361: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5086c8500 of size 294912 next 34\n",
      "2023-04-30 20:54:24.417364: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 508710500 of size 111673344 next 44\n",
      "2023-04-30 20:54:24.417367: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 50f190500 of size 55836672 next 43\n",
      "2023-04-30 20:54:24.417371: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5126d0500 of size 73728 next 93\n",
      "2023-04-30 20:54:24.417374: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5126e2500 of size 256 next 187\n",
      "2023-04-30 20:54:24.417377: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5126e2600 of size 256 next 185\n",
      "2023-04-30 20:54:24.417380: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5126e2700 of size 256 next 192\n",
      "2023-04-30 20:54:24.417382: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5126e2800 of size 256 next 193\n",
      "2023-04-30 20:54:24.417385: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5126e2900 of size 1280 next 186\n",
      "2023-04-30 20:54:24.417388: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5126e2e00 of size 1280 next 197\n",
      "2023-04-30 20:54:24.417392: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5126e3300 of size 256 next 201\n",
      "2023-04-30 20:54:24.417395: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5126e3400 of size 256 next 199\n",
      "2023-04-30 20:54:24.417398: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5126e3500 of size 256 next 190\n",
      "2023-04-30 20:54:24.417400: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5126e3600 of size 256 next 200\n",
      "2023-04-30 20:54:24.417403: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5126e3700 of size 256 next 196\n",
      "2023-04-30 20:54:24.417407: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5126e3800 of size 256 next 188\n",
      "2023-04-30 20:54:24.417410: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5126e3900 of size 73728 next 202\n",
      "2023-04-30 20:54:24.417414: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5126f5900 of size 109568 next 57\n",
      "2023-04-30 20:54:24.417417: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 512710500 of size 262144 next 56\n",
      "2023-04-30 20:54:24.417420: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 512750500 of size 1048576 next 63\n",
      "2023-04-30 20:54:24.417423: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 512850500 of size 524288 next 62\n",
      "2023-04-30 20:54:24.417425: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5128d0500 of size 1746944 next 123\n",
      "2023-04-30 20:54:24.417428: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 512a7ad00 of size 1746944 next 68\n",
      "2023-04-30 20:54:24.417431: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 512c25500 of size 1746944 next 67\n",
      "2023-04-30 20:54:24.417433: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 512dcfd00 of size 294912 next 100\n",
      "2023-04-30 20:54:24.417436: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 512e17d00 of size 196608 next 111\n",
      "2023-04-30 20:54:24.417439: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 512e47d00 of size 196608 next 112\n",
      "2023-04-30 20:54:24.417441: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 512e77d00 of size 262144 next 115\n",
      "2023-04-30 20:54:24.417444: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 512eb7d00 of size 262144 next 116\n",
      "2023-04-30 20:54:24.417446: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 512ef7d00 of size 524288 next 119\n",
      "2023-04-30 20:54:24.417449: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 512f77d00 of size 524288 next 120\n",
      "2023-04-30 20:54:24.417452: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 512ff7d00 of size 73728 next 158\n",
      "2023-04-30 20:54:24.417454: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 513009d00 of size 393216 next 179\n",
      "2023-04-30 20:54:24.417457: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 513069d00 of size 196608 next 165\n",
      "2023-04-30 20:54:24.417459: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 513099d00 of size 310272 next 83\n",
      "2023-04-30 20:54:24.417462: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5130e5900 of size 73182208 next 48\n",
      "2023-04-30 20:54:24.417465: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5176b0500 of size 83755008 next 47\n",
      "2023-04-30 20:54:24.417468: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51c690500 of size 2238464 next 88\n",
      "2023-04-30 20:54:24.417471: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51c8b2d00 of size 1310208 next 134\n",
      "2023-04-30 20:54:24.417474: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51c9f2b00 of size 1999616 next 198\n",
      "2023-04-30 20:54:24.417476: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51cbdae00 of size 793856 next 189\n",
      "2023-04-30 20:54:24.417479: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51cc9cb00 of size 2172928 next 143\n",
      "2023-04-30 20:54:24.417482: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51ceaf300 of size 1949696 next 146\n",
      "2023-04-30 20:54:24.417485: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d08b300 of size 1310208 next 149\n",
      "2023-04-30 20:54:24.417487: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d1cb100 of size 627968 next 145\n",
      "2023-04-30 20:54:24.417490: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d264600 of size 256 next 203\n",
      "2023-04-30 20:54:24.417493: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d264700 of size 256 next 204\n",
      "2023-04-30 20:54:24.417495: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d264800 of size 256 next 205\n",
      "2023-04-30 20:54:24.417498: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d264900 of size 256 next 206\n",
      "2023-04-30 20:54:24.417501: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d264a00 of size 256 next 207\n",
      "2023-04-30 20:54:24.417503: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d264b00 of size 256 next 208\n",
      "2023-04-30 20:54:24.417506: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d264c00 of size 512 next 210\n",
      "2023-04-30 20:54:24.417509: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d264e00 of size 512 next 211\n",
      "2023-04-30 20:54:24.417511: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d265000 of size 512 next 212\n",
      "2023-04-30 20:54:24.417514: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d265200 of size 512 next 213\n",
      "2023-04-30 20:54:24.417516: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d265400 of size 512 next 214\n",
      "2023-04-30 20:54:24.417519: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d265600 of size 512 next 215\n",
      "2023-04-30 20:54:24.417521: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d265800 of size 768 next 216\n",
      "2023-04-30 20:54:24.417524: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d265b00 of size 768 next 217\n",
      "2023-04-30 20:54:24.417527: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d265e00 of size 256000 next 180\n",
      "2023-04-30 20:54:24.417530: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d2a4600 of size 262144 next 178\n",
      "2023-04-30 20:54:24.417532: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d2e4600 of size 262144 next 221\n",
      "2023-04-30 20:54:24.417535: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d324600 of size 1024 next 222\n",
      "2023-04-30 20:54:24.417537: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d324a00 of size 1024 next 223\n",
      "2023-04-30 20:54:24.417540: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d324e00 of size 784384 next 184\n",
      "2023-04-30 20:54:24.417543: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d3e4600 of size 524288 next 183\n",
      "2023-04-30 20:54:24.417545: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d464600 of size 524288 next 224\n",
      "2023-04-30 20:54:24.417548: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d4e4600 of size 2048 next 225\n",
      "2023-04-30 20:54:24.417551: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d4e4e00 of size 2048 next 226\n",
      "2023-04-30 20:54:24.417553: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d4e5600 of size 2816 next 227\n",
      "2023-04-30 20:54:24.417556: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d4e6100 of size 2816 next 228\n",
      "2023-04-30 20:54:24.417561: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d4e6c00 of size 256 next 229\n",
      "2023-04-30 20:54:24.417564: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d4e6d00 of size 1280 next 230\n",
      "2023-04-30 20:54:24.417567: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d4e7200 of size 256 next 231\n",
      "2023-04-30 20:54:24.417569: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d4e7300 of size 256 next 232\n",
      "2023-04-30 20:54:24.417572: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d4e7400 of size 256 next 233\n",
      "2023-04-30 20:54:24.417574: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d4e7500 of size 256 next 234\n",
      "2023-04-30 20:54:24.417577: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d4e7600 of size 256 next 235\n",
      "2023-04-30 20:54:24.417580: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d4e7700 of size 256 next 236\n",
      "2023-04-30 20:54:24.417582: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d4e7800 of size 256 next 260\n",
      "2023-04-30 20:54:24.417585: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 51d4e7900 of size 768 next 258\n",
      "2023-04-30 20:54:24.417587: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d4e7c00 of size 512 next 269\n",
      "2023-04-30 20:54:24.417590: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d4e7e00 of size 256 next 241\n",
      "2023-04-30 20:54:24.417593: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d4e7f00 of size 256 next 239\n",
      "2023-04-30 20:54:24.417595: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d4e8000 of size 77568 next 271\n",
      "2023-04-30 20:54:24.417598: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d4faf00 of size 512 next 263\n",
      "2023-04-30 20:54:24.417601: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 51d4fb100 of size 534016 next 152\n",
      "2023-04-30 20:54:24.417603: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 51d57d700 of size 61569536 next 91\n",
      "2023-04-30 20:54:24.417607: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 521035100 of size 1310208 next 86\n",
      "2023-04-30 20:54:24.417609: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 521174f00 of size 83755008 next 107\n",
      "2023-04-30 20:54:24.417612: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 526154f00 of size 83755008 next 108\n",
      "2023-04-30 20:54:24.417615: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 52b134f00 of size 89653248 next 172\n",
      "2023-04-30 20:54:24.417617: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5306b4f00 of size 44826624 next 171\n",
      "2023-04-30 20:54:24.417620: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 533174f00 of size 79468544 next 195\n",
      "2023-04-30 20:54:24.417623: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 537d3e700 of size 294912 next 209\n",
      "2023-04-30 20:54:24.417626: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 537d86700 of size 196608 next 218\n",
      "2023-04-30 20:54:24.417628: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 537db6700 of size 1024 next 219\n",
      "2023-04-30 20:54:24.417631: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 537db6b00 of size 1024 next 220\n",
      "2023-04-30 20:54:24.417637: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 537db6f00 of size 434176 next 191\n",
      "2023-04-30 20:54:24.417640: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 537e20f00 of size 1153024 next 194\n",
      "2023-04-30 20:54:24.417643: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 537f3a700 of size 2750720 next 261\n",
      "2023-04-30 20:54:24.417646: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5381da000 of size 1171200 next 162\n",
      "2023-04-30 20:54:24.417649: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 5382f7f00 of size 4046848 next 147\n",
      "2023-04-30 20:54:24.417652: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5386d3f00 of size 85163520 next 144\n",
      "2023-04-30 20:54:24.417655: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 53d80bd00 of size 67239936 next 175\n",
      "2023-04-30 20:54:24.417658: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 54182bd00 of size 1702272000 next 250\n",
      "2023-04-30 20:54:24.417660: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 5a6f95900 of size 1702272000 next 256\n",
      "2023-04-30 20:54:24.417663: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 60c6ff500 of size 1648665600 next 249\n",
      "2023-04-30 20:54:24.417666: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 66eb49900 of size 433831936 next 254\n",
      "2023-04-30 20:54:24.417668: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 688905900 of size 53196032 next 242\n",
      "2023-04-30 20:54:24.417671: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 68bbc0e00 of size 53196032 next 243\n",
      "2023-04-30 20:54:24.417674: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 68ee7c300 of size 1151232 next 159\n",
      "2023-04-30 20:54:24.417677: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 68ef95400 of size 40118784 next 259\n",
      "2023-04-30 20:54:24.417679: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 6915d7e00 of size 1478656 next 252\n",
      "2023-04-30 20:54:24.417682: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 691740e00 of size 268647424 next 280\n",
      "2023-04-30 20:54:24.417686: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 6a1774a00 of size 1702272000 next 266\n",
      "2023-04-30 20:54:24.417689: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 706ede600 of size 53196032 next 237\n",
      "2023-04-30 20:54:24.417691: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 70a199b00 of size 74829056 next 282\n",
      "2023-04-30 20:54:24.417694: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 70e8f6800 of size 1324390400 next 18446744073709551615\n",
      "2023-04-30 20:54:24.417699: I tensorflow/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2023-04-30 20:54:24.417705: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 120 Chunks of size 256 totalling 30.0KiB\n",
      "2023-04-30 20:54:24.417709: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 24 Chunks of size 512 totalling 12.0KiB\n",
      "2023-04-30 20:54:24.417712: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 8 Chunks of size 768 totalling 6.0KiB\n",
      "2023-04-30 20:54:24.417715: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 12 Chunks of size 1024 totalling 12.0KiB\n",
      "2023-04-30 20:54:24.417719: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 8 Chunks of size 1280 totalling 10.0KiB\n",
      "2023-04-30 20:54:24.417722: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1536 totalling 1.5KiB\n",
      "2023-04-30 20:54:24.417725: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 6 Chunks of size 2048 totalling 12.0KiB\n",
      "2023-04-30 20:54:24.417728: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 2816 totalling 5.5KiB\n",
      "2023-04-30 20:54:24.417731: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 3584 totalling 10.5KiB\n",
      "2023-04-30 20:54:24.417734: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 4608 totalling 4.5KiB\n",
      "2023-04-30 20:54:24.417737: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 4 Chunks of size 73728 totalling 288.0KiB\n",
      "2023-04-30 20:54:24.417740: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 77568 totalling 75.8KiB\n",
      "2023-04-30 20:54:24.417743: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 80384 totalling 78.5KiB\n",
      "2023-04-30 20:54:24.417746: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 109568 totalling 107.0KiB\n",
      "2023-04-30 20:54:24.417750: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 5 Chunks of size 196608 totalling 960.0KiB\n",
      "2023-04-30 20:54:24.417753: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 256000 totalling 250.0KiB\n",
      "2023-04-30 20:54:24.417756: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 5 Chunks of size 262144 totalling 1.25MiB\n",
      "2023-04-30 20:54:24.417759: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 294912 totalling 864.0KiB\n",
      "2023-04-30 20:54:24.417762: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 310272 totalling 303.0KiB\n",
      "2023-04-30 20:54:24.417765: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 393216 totalling 768.0KiB\n",
      "2023-04-30 20:54:24.417768: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 434176 totalling 424.0KiB\n",
      "2023-04-30 20:54:24.417771: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 5 Chunks of size 524288 totalling 2.50MiB\n",
      "2023-04-30 20:54:24.417774: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 627968 totalling 613.2KiB\n",
      "2023-04-30 20:54:24.417777: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 784384 totalling 766.0KiB\n",
      "2023-04-30 20:54:24.417780: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 793856 totalling 775.2KiB\n",
      "2023-04-30 20:54:24.417783: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1048576 totalling 1.00MiB\n",
      "2023-04-30 20:54:24.417786: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1151232 totalling 1.10MiB\n",
      "2023-04-30 20:54:24.417789: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1153024 totalling 1.10MiB\n",
      "2023-04-30 20:54:24.417808: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1171200 totalling 1.12MiB\n",
      "2023-04-30 20:54:24.417815: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 1310208 totalling 3.75MiB\n",
      "2023-04-30 20:54:24.417818: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1478656 totalling 1.41MiB\n",
      "2023-04-30 20:54:24.417821: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 1746944 totalling 5.00MiB\n",
      "2023-04-30 20:54:24.417824: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1949696 totalling 1.86MiB\n",
      "2023-04-30 20:54:24.417827: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1999616 totalling 1.91MiB\n",
      "2023-04-30 20:54:24.417830: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 2172928 totalling 2.07MiB\n",
      "2023-04-30 20:54:24.417833: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 2238464 totalling 2.13MiB\n",
      "2023-04-30 20:54:24.417836: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 44826624 totalling 42.75MiB\n",
      "2023-04-30 20:54:24.417840: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 53196032 totalling 101.46MiB\n",
      "2023-04-30 20:54:24.417843: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 55836672 totalling 53.25MiB\n",
      "2023-04-30 20:54:24.417846: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 61569536 totalling 58.72MiB\n",
      "2023-04-30 20:54:24.417849: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 67239936 totalling 64.12MiB\n",
      "2023-04-30 20:54:24.417852: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 73182208 totalling 69.79MiB\n",
      "2023-04-30 20:54:24.417855: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 74829056 totalling 71.36MiB\n",
      "2023-04-30 20:54:24.417858: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 79468544 totalling 75.79MiB\n",
      "2023-04-30 20:54:24.417862: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 83755008 totalling 239.62MiB\n",
      "2023-04-30 20:54:24.417865: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 85163520 totalling 81.22MiB\n",
      "2023-04-30 20:54:24.417868: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 89653248 totalling 85.50MiB\n",
      "2023-04-30 20:54:24.417871: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 111673344 totalling 106.50MiB\n",
      "2023-04-30 20:54:24.417874: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 433831936 totalling 413.73MiB\n",
      "2023-04-30 20:54:24.417877: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 1702272000 totalling 4.76GiB\n",
      "2023-04-30 20:54:24.417880: I tensorflow/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 6.22GiB\n",
      "2023-04-30 20:54:24.417883: I tensorflow/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 10018095104 memory_limit_: 10018095104 available bytes: 0 curr_region_allocation_bytes_: 20036190208\n",
      "2023-04-30 20:54:24.417898: I tensorflow/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                     10018095104\n",
      "InUse:                      6675744512\n",
      "MaxInUse:                   9509369088\n",
      "NumAllocs:                        5840\n",
      "MaxAllocSize:               2908389376\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-04-30 20:54:24.417907: W tensorflow/tsl/framework/bfc_allocator.cc:497] ********************************************________________******__*******************_____________\n",
      "2023-04-30 20:54:24.419465: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at pooling_ops_common.cc:624 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[300,32,65,682] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-04-30 20:54:24.422930: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[300,32,65,682] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node gradient_tape/model_2/average_pooling2d_5/AvgPoolGrad}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradient_tape/model_2/average_pooling2d_5/AvgPoolGrad' defined at (most recent call last):\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_4758/2943329651.py\", line 31, in <module>\n      model.fit(\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/wandb/integration/keras/keras.py\", line 174, in new_v2\n      return old_v2(*args, **kwargs)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/wandb/integration/keras/keras.py\", line 174, in new_v2\n      return old_v2(*args, **kwargs)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/wandb/integration/keras/keras.py\", line 174, in new_v2\n      return old_v2(*args, **kwargs)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/engine/training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/optimizers/optimizer.py\", line 542, in minimize\n      grads_and_vars = self.compute_gradients(loss, var_list, tape)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/optimizers/optimizer.py\", line 275, in compute_gradients\n      grads = tape.gradient(loss, var_list)\nNode: 'gradient_tape/model_2/average_pooling2d_5/AvgPoolGrad'\nOOM when allocating tensor with shape[300,32,65,682] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/model_2/average_pooling2d_5/AvgPoolGrad}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_7781]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 31\u001b[0m\n\u001b[1;32m      4\u001b[0m checkpoint_callback \u001b[39m=\u001b[39m WandbModelCheckpoint(\n\u001b[1;32m      5\u001b[0m     filepath\u001b[39m=\u001b[39mcheckpoint_path,\n\u001b[1;32m      6\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     save_freq\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m metrics_callback \u001b[39m=\u001b[39m WandbCallback(\n\u001b[1;32m     15\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     log_batch_frequency\u001b[39m=\u001b[39mHYPER_PARAMS[\u001b[39m\"\u001b[39m\u001b[39mlogs-batch-frequency\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     32\u001b[0m     train_data,\n\u001b[1;32m     33\u001b[0m     epochs\u001b[39m=\u001b[39;49mHYPER_PARAMS[\u001b[39m\"\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     34\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m     35\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     36\u001b[0m         metrics_callback,\n\u001b[1;32m     37\u001b[0m         checkpoint_callback,\n\u001b[1;32m     38\u001b[0m     ],\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m run\u001b[39m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/qwe/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/model_2/average_pooling2d_5/AvgPoolGrad' defined at (most recent call last):\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_4758/2943329651.py\", line 31, in <module>\n      model.fit(\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/wandb/integration/keras/keras.py\", line 174, in new_v2\n      return old_v2(*args, **kwargs)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/wandb/integration/keras/keras.py\", line 174, in new_v2\n      return old_v2(*args, **kwargs)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/wandb/integration/keras/keras.py\", line 174, in new_v2\n      return old_v2(*args, **kwargs)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/engine/training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/optimizers/optimizer.py\", line 542, in minimize\n      grads_and_vars = self.compute_gradients(loss, var_list, tape)\n    File \"/home/user/anaconda3/envs/qwe/lib/python3.8/site-packages/keras/optimizers/optimizer.py\", line 275, in compute_gradients\n      grads = tape.gradient(loss, var_list)\nNode: 'gradient_tape/model_2/average_pooling2d_5/AvgPoolGrad'\nOOM when allocating tensor with shape[300,32,65,682] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/model_2/average_pooling2d_5/AvgPoolGrad}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_7781]"
     ]
    }
   ],
   "source": [
    "run = wandb.init(config=HYPER_PARAMS, project=\"speech-filter\")\n",
    "checkpoint_path = wandb.run.dir + \"/model_checkpoint.hdf5\"\n",
    "\n",
    "checkpoint_callback = WandbModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\",\n",
    ")\n",
    "\n",
    "metrics_callback = WandbCallback(\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    save_model=True,\n",
    "    save_graph=True,\n",
    "    save_weights_only=False,\n",
    "    log_weights=True,\n",
    "    log_gradients=True,\n",
    "    training_data=train_data,\n",
    "    validation_data=validation_data,\n",
    "    predictions=64,\n",
    "    input_type=\"images\",\n",
    "    output_type=\"image\",\n",
    "    log_batch_frequency=HYPER_PARAMS[\"logs-batch-frequency\"],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_data,\n",
    "    epochs=HYPER_PARAMS[\"epochs\"],\n",
    "    validation_data=validation_data,\n",
    "    callbacks=[\n",
    "        metrics_callback,\n",
    "        checkpoint_callback,\n",
    "    ],\n",
    ")\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
