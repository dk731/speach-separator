{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 19:20:06.333880: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-29 19:20:06.898970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-04-29 19:20:07.658692: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-29 19:20:07.679916: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-29 19:20:07.679991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import kaldiio\n",
    "\n",
    "from scipy.signal import welch\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "# conda install -c conda-forge tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = os.getenv(\"PROJECT_ROOT\")\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "CLIPS_PATH = os.getenv(\"CLIPS_PATH\")\n",
    "\n",
    "VALIDATED_LIST_PATH = os.path.join(os.getenv(\"CLIPS_META_PATH\"), \"validated.tsv\")\n",
    "XVECTOR_RESULT_PATH = os.getenv(\"XVECTOR_RESULT_PATH\")\n",
    "XVECTOR_SCP_PATH = os.path.join(XVECTOR_RESULT_PATH, \"xvector.scp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_xvectors = kaldiio.load_scp(XVECTOR_SCP_PATH)\n",
    "valid_speakers = set(speakers_xvectors.keys())\n",
    "\n",
    "raw_clips_meta = pd.read_table(VALIDATED_LIST_PATH)\n",
    "raw_clips_meta = raw_clips_meta[raw_clips_meta[\"client_id\"].isin(valid_speakers)]\n",
    "\n",
    "\n",
    "def get_path(row):\n",
    "    return f\"{CLIPS_PATH}/{row}.mp3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    Concatenate,\n",
    "    LeakyReLU,\n",
    "    BatchNormalization,\n",
    "    Flatten,\n",
    "    AveragePooling2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 19:20:09.249430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-29 19:20:09.249559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-29 19:20:09.249610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-29 19:20:10.023073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-29 19:20:10.023166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-29 19:20:10.023174: I tensorflow/core/co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " mel_spectrogram_input (InputLa  [(None, 65, 853, 1)  0          []                               \n",
      " yer)                           ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 65, 853, 32)  320         ['mel_spectrogram_input[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 65, 853, 32)  128        ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 32, 426, 32)  0          ['batch_normalization_1[0][0]']  \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 426, 64)  18496       ['average_pooling2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 426, 64)  256        ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 16, 213, 64)  0          ['batch_normalization_2[0][0]']  \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 16, 213, 128  73856       ['average_pooling2d_2[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 16, 213, 128  512        ['conv2d_3[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " average_pooling2d_3 (AveragePo  (None, 8, 106, 128)  0          ['batch_normalization_3[0][0]']  \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 108544)       0           ['average_pooling2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " x_vector_input (InputLayer)    [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 109056)       0           ['flatten[0][0]',                \n",
      "                                                                  'x_vector_input[0][0]']         \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          13959296    ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          33024       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 256)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 256)          65792       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 256)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 512)          131584      ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 853)          437589      ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,720,853\n",
      "Trainable params: 14,720,405\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mmon_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-29 19:20:10.023222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-29 19:20:10.023252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9554 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "HYPER_PARAMS = {\n",
    "    # Model parameters\n",
    "    \"window-width\": 65,\n",
    "    \"x-vector-dim\": 512,\n",
    "    # Training parameters\n",
    "    \"batch-size\": 32,\n",
    "    \"epochs\": 100,\n",
    "    \"learning-rate\": 0.0001,\n",
    "    \"learn-test-split\": 0.8,\n",
    "    # FFT parameters\n",
    "    \"nfft\": 8192,\n",
    "    \"fft-window\": 4096,\n",
    "    \"fft-stride\": 512,\n",
    "    # General\n",
    "    \"min-frequency\": 0,\n",
    "    \"max-frequency\": 10e3,\n",
    "    \"audio-rate\": 48e3,\n",
    "}\n",
    "\n",
    "HYPER_PARAMS[\"window-height\"] = int(\n",
    "    (HYPER_PARAMS[\"max-frequency\"] - HYPER_PARAMS[\"min-frequency\"])\n",
    "    / HYPER_PARAMS[\"audio-rate\"]\n",
    "    * (HYPER_PARAMS[\"nfft\"] // 2 + 1)\n",
    ")\n",
    "\n",
    "# Model parameters\n",
    "mel_spectrogram_shape = (\n",
    "    HYPER_PARAMS[\"window-width\"],\n",
    "    HYPER_PARAMS[\"window-height\"],\n",
    "    1\n",
    ")  # Replace window_size and num_mel_bands with your values\n",
    "\n",
    "# Leaky ReLU activation function\n",
    "leaky_relu = LeakyReLU(alpha=0.2)\n",
    "\n",
    "# Mel-spectrogram input\n",
    "mel_spectrogram_input = Input(shape=mel_spectrogram_shape, name=\"mel_spectrogram_input\")\n",
    "\n",
    "# Convolutional layers\n",
    "x = Conv2D(16, (3, 3), padding=\"same\", activation=leaky_relu)(mel_spectrogram_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Convolutional layers\n",
    "x = Conv2D(32, (3, 3), padding=\"same\", activation=leaky_relu)(mel_spectrogram_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), padding=\"same\", activation=leaky_relu)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(128, (3, 3), padding=\"same\", activation=leaky_relu)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "# X-vector input\n",
    "x_vector_input = Input(shape=(HYPER_PARAMS[\"x-vector-dim\"],), name=\"x_vector_input\")\n",
    "\n",
    "# Concatenate flattened CNN output with x-vector input\n",
    "combined_input = Concatenate()([x, x_vector_input])\n",
    "\n",
    "# Dense layers\n",
    "y = Dense(128, activation=leaky_relu)(combined_input)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Dense(256, activation=leaky_relu)(y)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Dense(256, activation=leaky_relu)(y)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Dense(512, activation=leaky_relu)(y)\n",
    "output = Dense(HYPER_PARAMS[\"window-height\"], activation=\"linear\")(y)\n",
    "\n",
    "# Construct the model\n",
    "model = Model(inputs=[mel_spectrogram_input, x_vector_input], outputs=output)\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=HYPER_PARAMS[\"learning-rate\"])\n",
    "loss_fn = MeanSquaredError()\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "class SamplesLoader:\n",
    "    \n",
    "    def __init__(self, audio_samples, x_vectors, modifications, params):\n",
    "        self.audio_samples = audio_samples\n",
    "        self.x_vectors = x_vectors\n",
    "\n",
    "        self.batch_size = params[\"batch-size\"]\n",
    "        self.split = params[\"learn-test-split\"]\n",
    "        self.nfft = params[\"nfft\"]\n",
    "        self.fft_window = params[\"fft-window\"]\n",
    "        self.fft_stride = params[\"fft-stride\"]\n",
    "\n",
    "        self.min_frequency = params[\"min-frequency\"]\n",
    "        self.max_frequency = params[\"max-frequency\"]\n",
    "        self.audio_rate = params[\"audio-rate\"]\n",
    "\n",
    "        self.window_width = params[\"window-width\"]\n",
    "        self.window_height = params[\"window-height\"]\n",
    "\n",
    "        self.min_frequency = params[\"min-frequency\"]\n",
    "        self.max_frequency = params[\"max-frequency\"]\n",
    "\n",
    "        self.x_vector_size = params[\"x-vector-dim\"]\n",
    "\n",
    "        self.clip_start_index = int(self.min_frequency / self.audio_rate * self.fft_window // 2)\n",
    "        self.clip_end_index = self.clip_start_index + self.window_height\n",
    "\n",
    "        # Modification are dict where key is possibility of modification\n",
    "        # and value is callback that accepts raw sample and returns modified sample\n",
    "        self.modifications = modifications\n",
    "\n",
    "        self.active_batch = {\n",
    "            \"input\": tf.zeros(shape=(0, self.window_width, self.window_height)),\n",
    "            \"x-vector\": tf.zeros(shape=(0, self.x_vector_size)),\n",
    "            \"output\": tf.zeros(shape=(0, self.window_height)),\n",
    "        }\n",
    "\n",
    "        self.active_samples_iter = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.active_samples_iter = self.audio_samples.sample(frac=1).iterrows()\n",
    "        return self\n",
    "\n",
    "    def _slice_batch(self):\n",
    "        if self.active_batch[\"input\"].shape[0] < self.batch_size:\n",
    "            return None\n",
    "\n",
    "        new_batch = dict()\n",
    "        for key in self.active_batch.keys():\n",
    "            new_batch[key] = self.active_batch[key][: self.batch_size]\n",
    "            self.active_batch[key] = self.active_batch[key][self.batch_size :]\n",
    "\n",
    "        return new_batch[\"input\"], new_batch[\"x-vector\"], new_batch[\"output\"]\n",
    "\n",
    "\n",
    "    def _get_spectrogram(self, audio_tensor):\n",
    "        spectrogram = tfio.audio.spectrogram(\n",
    "            audio_tensor,\n",
    "            nfft=self.nfft,\n",
    "            window=self.fft_window,\n",
    "            stride=self.fft_stride,\n",
    "        )\n",
    "\n",
    "        # Slice away frequencies outside of the human voice range\n",
    "        sliced_tensor = tf.slice(spectrogram, [0, self.clip_start_index], [spectrogram.shape[0], self.clip_end_index])\n",
    "        mean = tf.math.reduce_mean(sliced_tensor)\n",
    "        std_dev = tf.math.reduce_std(sliced_tensor)\n",
    "        final_tensor = (sliced_tensor - mean) / std_dev  # Normalize the tensor\n",
    "\n",
    "        return final_tensor\n",
    "\n",
    "    def _process_audio_tensor(self, audio_tensor):\n",
    "        final_tensor = self._get_spectrogram(audio_tensor)  # Get normalized tensor\n",
    "\n",
    "        # Calculate padding width\n",
    "        pad_width = (self.window_width - 1) // 2\n",
    "\n",
    "        # Pad the input image on both sides along the width axis\n",
    "        padded_image = tf.pad(final_tensor, [[pad_width, pad_width], [0, 0]])\n",
    "\n",
    "        # Extract patches with a sliding window\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=tf.expand_dims(tf.expand_dims(padded_image, -1), 0),  # Add a batch dimension to the input image\n",
    "            sizes=[1, self.window_width, self.window_height, 1],           # Patch size (1, w_w, h, 1)\n",
    "            strides=[1, 1, 1, 1],                    # Stride (1, 1, 1, 1) for a sliding window with a step of 1\n",
    "            rates=[1, 1, 1, 1],                      # Dilation rate (1, 1, 1, 1)\n",
    "            padding='VALID'                          # No padding is required as we already padded the input image\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        # Reshape the patches tensor to the desired output shape (w, w_w, h)\n",
    "        patches = tf.reshape(patches, [final_tensor.shape[0], self.window_width, self.window_height])\n",
    "        return patches\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            existing_batch = self._slice_batch()\n",
    "            if existing_batch is not None:\n",
    "                return existing_batch\n",
    "\n",
    "            iter_result = self.active_samples_iter.__next__()\n",
    "            if iter_result is None:\n",
    "                return None # TODO: Pad left over samples with zeros\n",
    "\n",
    "            sample_index, new_sample = iter_result\n",
    "            sample_x_vector = self.x_vectors[new_sample[\"client_id\"]]\n",
    "            if sample_x_vector is None:\n",
    "                continue\n",
    "\n",
    "            # Load audio sample\n",
    "            audio = tfio.audio.AudioIOTensor(get_path(new_sample[\"path\"]))\n",
    "            if audio.shape[0] < 1000:  # Skip to short samples\n",
    "                continue\n",
    "\n",
    "            # Crop audio beggining and end to remove silence and stop/start button clicks\n",
    "            raw_audio_tensor = tf.squeeze(audio[300:-350], axis=[-1])\n",
    "            raw_audio_spectrogram = self._get_spectrogram(raw_audio_tensor)\n",
    "            tiled_x_vector = tf.tile(tf.expand_dims(sample_x_vector, 0), (raw_audio_spectrogram.shape[0], 1))\n",
    "\n",
    "            modified_samples = []\n",
    "            for possibility, callback in self.modifications.items():\n",
    "                if possibility > random():\n",
    "                    modified_samples.append(callback(raw_audio_tensor))\n",
    "\n",
    "            for modified_sample in modified_samples:\n",
    "                sample_patches = self._process_audio_tensor(modified_sample)\n",
    "\n",
    "                self.active_batch[\"input\"] = tf.concat(\n",
    "                    [self.active_batch[\"input\"], sample_patches], axis=0\n",
    "                )\n",
    "                self.active_batch[\"x-vector\"] = tf.concat(\n",
    "                    [self.active_batch[\"x-vector\"], tiled_x_vector], axis=0\n",
    "                )\n",
    "                self.active_batch[\"output\"] = tf.concat(\n",
    "                    [self.active_batch[\"output\"], raw_audio_spectrogram], axis=0\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e38e400bc049b080a61dd152ffa28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 19:22:35.090401: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype float and shape [32,853]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tensorboard_callback = TensorBoard(histogram_freq=1)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 50\n",
    "train_steps_per_epoch = 2000  # Adjust this value based on your training set size\n",
    "val_steps_per_epoch = 500  # Adjust this value based on your validation set size\n",
    "\n",
    "train_test_start_index = int(raw_clips_meta.shape[0] * HYPER_PARAMS[\"learn-test-split\"])\n",
    "train_dataset = raw_clips_meta.iloc[:train_test_start_index]\n",
    "test_dataset = raw_clips_meta.iloc[train_test_start_index:]\n",
    "\n",
    "train_loader = SamplesLoader(train_dataset, speakers_xvectors, {\n",
    "    1: lambda sample: sample,\n",
    "}, HYPER_PARAMS)\n",
    "\n",
    "test_loader = SamplesLoader(test_dataset, speakers_xvectors, {\n",
    "    1: lambda sample: sample,\n",
    "}, HYPER_PARAMS)\n",
    "\n",
    "for epoch in tqdm(range(HYPER_PARAMS[\"epochs\"]), desc=\"Training\"):\n",
    "    # Train on batches\n",
    "    for batch_X_mel, batch_X_xvec, batch_y in train_loader:\n",
    "        train_result = model.train_on_batch(\n",
    "            x=[batch_X_mel, batch_X_xvec], y=batch_y, reset_metrics=False\n",
    "        )\n",
    "\n",
    "        print(\"WEee\")\n",
    "\n",
    "        # Write train metrics to TensorBoard\n",
    "        with tensorboard_callback.as_default():\n",
    "            tf.summary.scalar(\"loss\", train_result, step=epoch)\n",
    "\n",
    "    # Validate on batches\n",
    "    # val_losses = []\n",
    "    # for batch_X_mel, batch_X_xvec, batch_y in val_dataset:\n",
    "    #     val_result = model.test_on_batch(x=[batch_X_mel, batch_X_xvec], y=batch_y)\n",
    "\n",
    "    #     # Collect validation losses\n",
    "    #     val_losses.append(val_result)\n",
    "\n",
    "    # Write validation metrics to TensorBoard\n",
    "    # mean_val_loss = np.mean(val_losses)\n",
    "    # with tensorboard_callback.as_default():\n",
    "    #     tf.summary.scalar(\"val_loss\", mean_val_loss, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
