{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import kaldiio\n",
    "\n",
    "from scipy.signal import welch\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = os.getenv(\"PROJECT_ROOT\")\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "CLIPS_PATH = os.getenv(\"CLIPS_PATH\")\n",
    "\n",
    "VALIDATED_LIST_PATH = os.path.join(os.getenv(\"CLIPS_META_PATH\"), \"validated.tsv\")\n",
    "XVECTOR_RESULT_PATH = os.getenv(\"XVECTOR_RESULT_PATH\")\n",
    "XVECTOR_SCP_PATH = os.path.join(XVECTOR_RESULT_PATH, \"xvector.scp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_xvectors = kaldiio.load_scp(XVECTOR_SCP_PATH)\n",
    "valid_speakers = set(speakers_xvectors.keys())\n",
    "\n",
    "raw_clips_meta = pd.read_table(VALIDATED_LIST_PATH)\n",
    "raw_clips_meta = raw_clips_meta[raw_clips_meta[\"client_id\"].isin(valid_speakers)]\n",
    "\n",
    "\n",
    "def get_path(row):\n",
    "    return f\"{CLIPS_PATH}/{row['path'].values[0]}.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert 6.283185307179586j to EagerTensor of dtype float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m linear_spectrogram_tensor \u001b[39m=\u001b[39m mel_to_linear(mel_spectrogram_tensor, sample_rate, n_fft, num_mel_bins)\n\u001b[1;32m     57\u001b[0m \u001b[39m# Apply Griffin-Lim algorithm to reconstruct the phase information (optional, but improves quality)\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m reconstructed_audio_tensor \u001b[39m=\u001b[39m griffin_lim(linear_spectrogram_tensor, n_fft, hop_length)\n\u001b[1;32m     60\u001b[0m \u001b[39m# Save the reconstructed audio as a WAV file\u001b[39;00m\n\u001b[1;32m     61\u001b[0m wav_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39maudio\u001b[39m.\u001b[39mencode_wav(reconstructed_audio_tensor[tf\u001b[39m.\u001b[39mnewaxis, :], sample_rate)\n",
      "Cell \u001b[0;32mIn[54], line 14\u001b[0m, in \u001b[0;36mgriffin_lim\u001b[0;34m(S, n_fft, hop_length, n_iter)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgriffin_lim\u001b[39m(S, n_fft, hop_length, n_iter\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m):\n\u001b[0;32m---> 14\u001b[0m     S_complex \u001b[39m=\u001b[39m S \u001b[39m*\u001b[39m tf\u001b[39m.\u001b[39mexp(\u001b[39m2\u001b[39;49mj \u001b[39m*\u001b[39;49m np\u001b[39m.\u001b[39;49mpi \u001b[39m*\u001b[39;49m tf\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49muniform(S\u001b[39m.\u001b[39;49mshape))\n\u001b[1;32m     15\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_iter):\n\u001b[1;32m     16\u001b[0m         x \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msignal\u001b[39m.\u001b[39minverse_stft(S_complex, frame_length\u001b[39m=\u001b[39mn_fft, frame_step\u001b[39m=\u001b[39mhop_length, fft_length\u001b[39m=\u001b[39mn_fft)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1194\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.r_binary_op_wrapper\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mr_binary_op_wrapper\u001b[39m(y, x):\n\u001b[1;32m   1193\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\u001b[39mNone\u001b[39;00m, op_name, [x, y]) \u001b[39mas\u001b[39;00m name:\n\u001b[0;32m-> 1194\u001b[0m     x \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(x, dtype\u001b[39m=\u001b[39;49my\u001b[39m.\u001b[39;49mdtype\u001b[39m.\u001b[39;49mbase_dtype, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1195\u001b[0m     \u001b[39mreturn\u001b[39;00m func(x, y, name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py:163\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1540\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1535\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor did not convert to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1536\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mthe preferred dtype: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m vs \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1537\u001b[0m                       (ret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype, preferred_dtype\u001b[39m.\u001b[39mbase_dtype))\n\u001b[1;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1540\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1542\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1543\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:339\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    337\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    338\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[0;32m--> 339\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:264\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    168\u001b[0m   \u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[1;32m    170\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    265\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:276\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    275\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 276\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    278\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[1;32m    279\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    300\u001b[0m   \u001b[39m\"\"\"Implementation of eager constant.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    302\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert 6.283185307179586j to EagerTensor of dtype float"
     ]
    }
   ],
   "source": [
    "def mel_to_linear(mel_spectrogram, sample_rate, n_fft, num_mel_bins):\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "        num_mel_bins=num_mel_bins,\n",
    "        num_spectrogram_bins=n_fft // 2 + 1,\n",
    "        sample_rate=sample_rate,\n",
    "        lower_edge_hertz=0.0,\n",
    "        upper_edge_hertz=sample_rate / 2.0,\n",
    "    )\n",
    "    # Invert the mel weight matrix\n",
    "    mel_to_linear_weight_matrix = tf.linalg.pinv(linear_to_mel_weight_matrix)\n",
    "    return tf.tensordot(mel_spectrogram, mel_to_linear_weight_matrix, 1)\n",
    "\n",
    "def griffin_lim(S, n_fft, hop_length, n_iter=32):\n",
    "    S_complex = S * tf.exp(2j * np.pi * tf.complex(tf.random.uniform(S.shape), 0.0))\n",
    "    for _ in range(n_iter):\n",
    "        x = tf.signal.inverse_stft(S_complex, frame_length=n_fft, frame_step=hop_length, fft_length=n_fft)\n",
    "        S_complex = tf.signal.stft(x, frame_length=n_fft, frame_step=hop_length)\n",
    "        S_complex = S_complex * tf.cast(S / (tf.abs(S_complex) + 1e-10), tf.complex64)\n",
    "    return x\n",
    "\n",
    "n_mels = 128\n",
    "n_fft = 1024\n",
    "n_stft = n_fft\n",
    "hop_length = 512\n",
    "win_length = n_fft\n",
    "num_mel_bins = 20\n",
    "\n",
    "sample_path = get_path(raw_clips_meta.sample())\n",
    "sample_data, sample_rate = torchaudio.load(sample_path, normalize=True)\n",
    "\n",
    "audio_tensor = tf.convert_to_tensor(sample_data)\n",
    "\n",
    "# Compute the STFT\n",
    "stft = tf.signal.stft(audio_tensor, frame_length=n_fft, frame_step=hop_length)\n",
    "\n",
    "# Compute the magnitude spectrogram\n",
    "magnitude_spectrogram = tf.abs(stft)\n",
    "\n",
    "# Compute the mel-spectrogram\n",
    "mel_filterbank = tf.signal.linear_to_mel_weight_matrix(\n",
    "    num_mel_bins=num_mel_bins,\n",
    "    num_spectrogram_bins=magnitude_spectrogram.shape[-1],\n",
    "    sample_rate=sample_rate,\n",
    "    lower_edge_hertz=0.0,\n",
    "    upper_edge_hertz=sample_rate / 2.0,\n",
    ")\n",
    "\n",
    "mel_spectrogram = tf.tensordot(magnitude_spectrogram, mel_filterbank, 1)\n",
    "\n",
    "# Assuming you have a mel-spectrogram stored in the variable `mel_spectrogram`\n",
    "# Convert it to a TensorFlow tensor if it's not already\n",
    "mel_spectrogram_tensor = tf.convert_to_tensor(mel_spectrogram)\n",
    "\n",
    "# Inverse mel-scale transformation\n",
    "linear_spectrogram_tensor = mel_to_linear(mel_spectrogram_tensor, sample_rate, n_fft, num_mel_bins)\n",
    "\n",
    "# Apply Griffin-Lim algorithm to reconstruct the phase information (optional, but improves quality)\n",
    "reconstructed_audio_tensor = griffin_lim(linear_spectrogram_tensor, n_fft, hop_length)\n",
    "\n",
    "# Save the reconstructed audio as a WAV file\n",
    "wav_data = tf.audio.encode_wav(reconstructed_audio_tensor[tf.newaxis, :], sample_rate)\n",
    "tf.io.write_file(\"reconstructed_audio.wav\", wav_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m invers_transform \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39mInverseMelScale(sample_rate\u001b[39m=\u001b[39msample_rate, n_stft\u001b[39m=\u001b[39mn_stft)\n\u001b[1;32m     24\u001b[0m grifflim_transform \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39mGriffinLim(n_fft\u001b[39m=\u001b[39mn_fft)\n\u001b[0;32m---> 26\u001b[0m inverse_waveform \u001b[39m=\u001b[39m invers_transform(mel_spectrogram)\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m2\u001b[39m)\n\u001b[1;32m     28\u001b[0m pseudo_waveform \u001b[39m=\u001b[39m grifflim_transform(inverse_waveform)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/torchaudio/transforms/_transforms.py:515\u001b[0m, in \u001b[0;36mInverseMelScale.forward\u001b[0;34m(self, melspec)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter):\n\u001b[1;32m    514\u001b[0m     optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 515\u001b[0m     diff \u001b[39m=\u001b[39m melspec \u001b[39m-\u001b[39m specgram\u001b[39m.\u001b[39;49mmatmul(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfb)\n\u001b[1;32m    516\u001b[0m     new_loss \u001b[39m=\u001b[39m diff\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m    517\u001b[0m     \u001b[39m# take sum over mel-frequency then average over other dimensions\u001b[39;00m\n\u001b[1;32m    518\u001b[0m     \u001b[39m# so that loss threshold is applied par unit timeframe\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_path = get_path(raw_clips_meta.sample())\n",
    "sample_data, sample_rate = torchaudio.load(sample_path, normalize=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# n_mels = 128\n",
    "# n_fft = 2048\n",
    "# n_stft = n_fft\n",
    "# hop_length = 512\n",
    "# win_length = n_fft\n",
    "\n",
    "\n",
    "# mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "#     sample_rate=sample_rate,\n",
    "#     n_fft=n_fft,\n",
    "#     win_length=win_length,\n",
    "#     hop_length=hop_length,\n",
    "#     n_mels=n_mels,\n",
    "# )(sample_data)\n",
    "\n",
    "# print(1)\n",
    "\n",
    "# transofrm = torchaudio.transforms.MelSpectrogram(sample_rate, n_fft=n_fft)\n",
    "# invers_transform = torchaudio.transforms.InverseMelScale(sample_rate=sample_rate, n_stft=n_stft)\n",
    "# grifflim_transform = torchaudio.transforms.GriffinLim(n_fft=n_fft)\n",
    "\n",
    "# inverse_waveform = invers_transform(mel_spectrogram)\n",
    "# print(2)\n",
    "# pseudo_waveform = grifflim_transform(inverse_waveform)\n",
    "# print(3)\n",
    "\n",
    "# torchaudio.save(\"~/original_audio.wav\", sample_data, sample_rate)\n",
    "# torchaudio.save(\"~/reconstructed_audio.wav\", pseudo_waveform.unsqueeze(0), sample_rate)\n",
    "\n",
    "\n",
    "# mel_spectrogram_db = torchaudio.transforms.AmplitudeToDB()(mel_spectrogram)\n",
    "\n",
    "# mel_spectrogram_db_mean = tf.reduce_mean(mel_spectrogram_db)\n",
    "# mel_spectrogram_db_std = tf.math.reduce_std(mel_spectrogram_db)\n",
    "# mel_spectrogram_normalized = (\n",
    "#     mel_spectrogram_db - mel_spectrogram_db_mean\n",
    "# ) / mel_spectrogram_db_std\n",
    "\n",
    "# # Plot the normalized mel-spectrogram\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# plt.imshow(\n",
    "#     mel_spectrogram_normalized.numpy().squeeze(),\n",
    "#     origin=\"lower\",\n",
    "#     aspect=\"auto\",\n",
    "#     cmap=\"viridis\",\n",
    "# )\n",
    "# plt.colorbar()\n",
    "# plt.title(\"Normalized Mel-Spectrogram\")\n",
    "# plt.xlabel(\"Time\")\n",
    "# plt.ylabel(\"Mel frequency\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save(\"original_audio.wav\", sample_data, sample_rate)\n",
    "torchaudio.save(\"reconstructed_audio.wav\", pseudo_waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_white_noise(audio_length, sr):\n",
    "    return np.random.normal(0, 1, audio_length)\n",
    "\n",
    "\n",
    "def generate_pink_noise(audio_length, sr):\n",
    "    num_freqs, _ = welch(np.ones(audio_length), fs=sr)\n",
    "    pink_spectrum = 1 / np.sqrt(np.linspace(1, sr // 2, num_freqs))\n",
    "    pink_noise = np.fft.irfft(\n",
    "        pink_spectrum * np.fft.rfft(generate_white_noise(audio_length, sr))\n",
    "    )\n",
    "    return pink_noise\n",
    "\n",
    "\n",
    "def add_noise(audio, snr, noise_type=\"white\", sr=None, noise_file=None):\n",
    "    audio_power = np.sum(audio**2)\n",
    "\n",
    "    if noise_type == \"white\":\n",
    "        noise = generate_white_noise(len(audio), sr)\n",
    "    elif noise_type == \"pink\":\n",
    "        noise = generate_pink_noise(len(audio), sr)\n",
    "    elif noise_type == \"file\":\n",
    "        if noise_file is None:\n",
    "            raise ValueError(\"noise_file must be specified for noise_type 'file'\")\n",
    "        noise, _ = librosa.load(noise_file, sr=sr)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid noise_type, choose from 'white', 'pink', or 'file'\")\n",
    "\n",
    "    noise_power = np.sum(noise**2)\n",
    "    target_noise_power = audio_power / (10 ** (snr / 10))\n",
    "    noise_scaling_factor = np.sqrt(target_noise_power / noise_power)\n",
    "    noisy_audio = audio + noise_scaling_factor * noise\n",
    "    return noisy_audio\n",
    "\n",
    "\n",
    "def mix_speakers(*clips):\n",
    "    max_duration = max([clip.shape[0] for clip in clips])\n",
    "    mixed_audio = np.zeros(max_duration)\n",
    "\n",
    "    for clip in clips:\n",
    "        padded_clip = np.pad(clip, (0, max_duration - clip.shape[0]), mode=\"constant\")\n",
    "        mixed_audio += padded_clip\n",
    "\n",
    "    return mixed_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    Concatenate,\n",
    "    LeakyReLU,\n",
    "    BatchNormalization,\n",
    "    Flatten,\n",
    "    AveragePooling2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mel_spectrogram_input (InputLay [(None, 64, 256, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, 256, 32)  320         mel_spectrogram_input[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 64, 256, 32)  128         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_19 (AveragePo (None, 32, 128, 32)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 128, 64)  18496       average_pooling2d_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 128, 64)  256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_20 (AveragePo (None, 16, 64, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 64, 128)  73856       average_pooling2d_20[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 64, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_21 (AveragePo (None, 8, 32, 128)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 32768)        0           average_pooling2d_21[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "x_vector_input (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 33280)        0           flatten_5[0][0]                  \n",
      "                                                                 x_vector_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 128)          4259968     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 256)          33024       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 256)          0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 256)          65792       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 256)          0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 512)          131584      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 256)          131328      dense_20[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,715,264\n",
      "Trainable params: 4,714,816\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "HYPER_PARAMS = {\n",
    "    # Model parameters\n",
    "    \"window-width\": 64,\n",
    "    \"mel-bands\": 256,\n",
    "    \"x-vector-dim\": 512,\n",
    "    # Training parameters\n",
    "    \"batch-size\": 32,\n",
    "    \"epochs\": 100,\n",
    "    \"learning-rate\": 0.0001,\n",
    "}\n",
    "\n",
    "# Model parameters\n",
    "mel_spectrogram_shape = (\n",
    "    HYPER_PARAMS[\"window-width\"],\n",
    "    HYPER_PARAMS[\"mel-bands\"],\n",
    "    1,\n",
    ")  # Replace window_size and num_mel_bands with your values\n",
    "\n",
    "# Leaky ReLU activation function\n",
    "leaky_relu = LeakyReLU(alpha=0.2)\n",
    "\n",
    "# Mel-spectrogram input\n",
    "mel_spectrogram_input = Input(shape=mel_spectrogram_shape, name=\"mel_spectrogram_input\")\n",
    "\n",
    "# Convolutional layers\n",
    "x = Conv2D(16, (3, 3), padding=\"same\", activation=leaky_relu)(mel_spectrogram_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Convolutional layers\n",
    "x = Conv2D(32, (3, 3), padding=\"same\", activation=leaky_relu)(mel_spectrogram_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), padding=\"same\", activation=leaky_relu)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(128, (3, 3), padding=\"same\", activation=leaky_relu)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "# X-vector input\n",
    "x_vector_input = Input(shape=(HYPER_PARAMS[\"x-vector-dim\"],), name=\"x_vector_input\")\n",
    "\n",
    "# Concatenate flattened CNN output with x-vector input\n",
    "combined_input = Concatenate()([x, x_vector_input])\n",
    "\n",
    "# Dense layers\n",
    "y = Dense(128, activation=leaky_relu)(combined_input)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Dense(256, activation=leaky_relu)(y)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Dense(256, activation=leaky_relu)(y)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Dense(512, activation=leaky_relu)(y)\n",
    "output = Dense(HYPER_PARAMS[\"mel-bands\"], activation=\"linear\")(y)\n",
    "\n",
    "# Construct the model\n",
    "model = Model(inputs=[mel_spectrogram_input, x_vector_input], outputs=output)\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=HYPER_PARAMS[\"learning-rate\"])\n",
    "loss_fn = MeanSquaredError()\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 00:13:19.662430: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2023-04-20 00:13:19.662490: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2023-04-20 00:13:19.663041: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2023-04-20 00:13:19.663194: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "tensorboard_callback = TensorBoard(histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_sample(base_sample):\n",
    "    pass\n",
    "\n",
    "\n",
    "def empty_sample(base_sample):\n",
    "    pass\n",
    "\n",
    "\n",
    "def noisy_sample(base_sample):\n",
    "    pass\n",
    "\n",
    "\n",
    "COMBINDATION_RANGE = [1, 6]  # Range of number of speakers to add\n",
    "\n",
    "\n",
    "def combined_sample(base_sample):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_COEFICIENTS = {\n",
    "    raw_sample: 0.8,\n",
    "    empty_sample: 0.1,\n",
    "    noisy_sample: 0.4,\n",
    "    combined_sample: 0.1,\n",
    "}\n",
    "\n",
    "dataset_samples = []\n",
    "for raw_sample in raw_clips_meta.iterrows():\n",
    "    sample_path = get_path(raw_clips_meta.sample())\n",
    "    sample_data, sample_rate = torchaudio.load(sample_path)\n",
    "\n",
    "    for sample_converter, probability in SAMPLE_COEFICIENTS.items():\n",
    "        if probability < random.random():\n",
    "            dataset_samples.append(sample_converter(sample_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 50\n",
    "train_steps_per_epoch = 2000  # Adjust this value based on your training set size\n",
    "val_steps_per_epoch = 500  # Adjust this value based on your validation set size\n",
    "\n",
    "for epoch in tqdm(range(HYPER_PARAMS[\"epochs\"]), desc=\"Training\"):\n",
    "    pass\n",
    "    # Train on batches\n",
    "    # for batch_X_mel, batch_X_xvec, batch_y in train_dataset:\n",
    "    #     train_result = model.train_on_batch(\n",
    "    #         x=[batch_X_mel, batch_X_xvec], y=batch_y, reset_metrics=False\n",
    "    #     )\n",
    "\n",
    "    #     # Write train metrics to TensorBoard\n",
    "    #     with tensorboard_callback.as_default():\n",
    "    #         tf.summary.scalar(\"loss\", train_result, step=epoch)\n",
    "\n",
    "    # Validate on batches\n",
    "    # val_losses = []\n",
    "    # for batch_X_mel, batch_X_xvec, batch_y in val_dataset:\n",
    "    #     val_result = model.test_on_batch(x=[batch_X_mel, batch_X_xvec], y=batch_y)\n",
    "\n",
    "    #     # Collect validation losses\n",
    "    #     val_losses.append(val_result)\n",
    "\n",
    "    # Write validation metrics to TensorBoard\n",
    "    # mean_val_loss = np.mean(val_losses)\n",
    "    # with tensorboard_callback.as_default():\n",
    "    #     tf.summary.scalar(\"val_loss\", mean_val_loss, step=epoch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
